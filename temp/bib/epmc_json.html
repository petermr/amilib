<html><head></head><body><ul><li><span class="pmcid"> PMC11193050, </span><span class="author_string"> Rossini M, Montanaro G, Montreuil O, Tarasov S., </span><span class="title"> "Towards computable taxonomic knowledge: Leveraging nanopublications for sharing new synonyms in the Madagascan genus &lt;i&gt;Helictopleurus&lt;/i&gt; (Coleoptera, Scarabaeinae).", </span><span><b> 12 </b></span><span> (2024): </span><span><i> Biodiversity data journal, </i></span><span class="page_info">PI e120304, </span><a href="https://doi.org/10.3897/bdj.12.e120304"> DOI: 10.3897/bdj.12.e120304, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Numerous taxonomic studies have focused on the dung beetle genus &lt;i&gt;Helictopleurus&lt;/i&gt; d'Orbigny, 1915, endemic to Madagascar. However, this genus stilll needs a thorough revision. Semantic technologies, such as nanopublications, hold the potential to enhance taxonomy by transforming how data are published and analysed. This paper evaluates the effectiveness of nanopublications in establishing synonyms within the genus &lt;i&gt;Helictopleurus&lt;/i&gt;.&lt;h4&gt;New information&lt;/h4&gt;In this study, we identify four new synonyms within &lt;i&gt;Helictopleurus&lt;/i&gt;: &lt;i&gt;H.rudicollis&lt;/i&gt; (Fairmaire, 1898) = &lt;i&gt;H.hypocrita&lt;/i&gt; Balthasar, 1941 &lt;b&gt;syn. nov.&lt;/b&gt;; &lt;i&gt;H.vadoni&lt;/i&gt; Lebis, 1960 = &lt;i&gt;H.perpunctatus&lt;/i&gt; Balthasar, 1963 &lt;b&gt;syn. nov.&lt;/b&gt;; &lt;i&gt;H.halffteri&lt;/i&gt; Balthasar, 1964 = &lt;i&gt;H.dorbignyi&lt;/i&gt; Montreuil, 2005 &lt;b&gt;syn. nov.&lt;/b&gt;; &lt;i&gt;H.clouei&lt;/i&gt; (Harold, 1869) = &lt;i&gt;H.gibbicollis&lt;/i&gt; (Fairmaire, 1895) &lt;b&gt;syn. nov.&lt;/b&gt; &lt;i&gt;Helictopleurus&lt;/i&gt; may have a significantly larger number of synonyms than currently known, indicating potentially inaccurate estimates about its recent extinction.We also publish the newly-established synonyms as nanopublications, which are machine-readable data snippets accessible online. Additionally, we explore the utility of nanopublications in taxonomy and demonstrate their practical use with an example query for data extraction."> Abstract: 158 words, </span></li><li><span class="pmcid"> PMC10280262, </span><span class="author_string"> Bucur CI, Kuhn T, Ceolin D, van Ossenbruggen J., </span><span class="title"> "Nanopublication-based semantic publishing and reviewing: a field study with formalization papers.", </span><span><b> 9 </b></span><span> (2023): </span><span><i> PeerJ. Computer science, </i></span><span class="page_info">PI e1159, </span><a href="https://doi.org/10.7717/peerj-cs.1159"> DOI: 10.7717/peerj-cs.1159, </a><span class="abstract_text" title="With the rapidly increasing amount of scientific literature, it is getting continuously more difficult for researchers in different disciplines to keep up-to-date with the recent findings in their field of study. Processing scientific articles in an automated fashion has been proposed as a solution to this problem, but the accuracy of such processing remains very poor for extraction tasks beyond the most basic ones (like locating and identifying entities and simple classification based on predefined categories). Few approaches have tried to change how we publish scientific results in the first place, such as by making articles machine-interpretable by expressing them with formal semantics from the start. In the work presented here, we propose a first step in this direction by setting out to demonstrate that we can formally publish high-level scientific claims in formal logic, and publish the results in a special issue of an existing journal. We use the concept and technology of nanopublications for this endeavor, and represent not just the submissions and final papers in this RDF-based format, but also the whole process in between, including reviews, responses, and decisions. We do this by performing a field study with what we call formalization papers, which contribute a novel formalization of a previously published claim. We received 15 submissions from 18 authors, who then went through the whole publication process leading to the publication of their contributions in the special issue. Our evaluation shows the technical and practical feasibility of our approach. The participating authors mostly showed high levels of interest and confidence, and mostly experienced the process as not very difficult, despite the technical nature of the current user interfaces. We believe that these results indicate that it is possible to publish scientific results from different fields with machine-interpretable semantics from the start, which in turn opens countless possibilities to radically improve in the future the effectiveness and efficiency of the scientific endeavor as a whole."> Abstract: 319 words, </span></li><li><span class="pmcid"> PMC11190572, </span><span class="author_string"> Montanaro G, Balhoff JP, Gir&#243;n JC, S&#246;derholm M, Tarasov S., </span><span class="title"> "Computable species descriptions and nanopublications: applying ontology-based technologies to dung beetles (Coleoptera, Scarabaeinae).", </span><span><b> 12 </b></span><span> (2024): </span><span><i> Biodiversity data journal, </i></span><span class="page_info">PI e121562, </span><a href="https://doi.org/10.3897/bdj.12.e121562"> DOI: 10.3897/bdj.12.e121562, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Taxonomy has long struggled with analysing vast amounts of phenotypic data due to computational and accessibility challenges. Ontology-based technologies provide a framework for modelling semantic phenotypes that are understandable by computers and compliant with FAIR principles. In this paper, we explore the use of Phenoscript, an emerging language designed for creating semantic phenotypes, to produce computable species descriptions. Our case study centers on the application of this approach to dung beetles (Coleoptera, Scarabaeinae).&lt;h4&gt;New information&lt;/h4&gt;We illustrate the effectiveness of Phenoscript for creating semantic phenotypes. We also demonstrate the ability of the Phenospy python package to automatically translate Phenoscript descriptions into natural language (NL), which eliminates the need for writing traditional NL descriptions. We introduce a computational pipeline that streamlines the generation of semantic descriptions and their conversion to NL. To demonstrate the power of the semantic approach, we apply simple semantic queries to the generated phenotypic descriptions. This paper addresses the current challenges in crafting semantic species descriptions and outlines the path towards future improvements. Furthermore, we discuss the promising integration of semantic phenotypes and nanopublications, as emerging methods for sharing scientific information. Overall, our study highlights the pivotal role of ontology-based technologies in modernising taxonomy and aligning it with the evolving landscape of big data analysis and FAIR principles."> Abstract: 209 words, </span></li><li><span class="pmcid"> PMC10414429, </span><span class="title"> "Peer Review of &#8220;Representing Physician Suicide Claims as Nanopublications: Proof-of-Concept Study Creating Claim Networks&#8221;", </span><span><b> 3(3) </b></span><span> (2022): </span><span><i> JMIRx med, </i></span></li><li><span class="pmcid"> PMC10414380, </span><span class="title"> "Peer Review of &#8220;Representing Physician Suicide Claims as Nanopublications: Proof-of-Concept Study Creating Claim Networks&#8221;", </span><span><b> 3(3) </b></span><span> (2022): </span><span><i> JMIRx med, </i></span></li><li><span class="pmcid"> PMC10414501, </span><span class="author_string"> Soares Teles A., </span><span class="title"> "Peer Review of &#8220;Representing Physician Suicide Claims as Nanopublications: Proof-of-Concept Study Creating Claim Networks&#8221;", </span><span><b> 3(3) </b></span><span> (2022): </span><span><i> JMIRx med, </i></span></li><li><span class="pmcid"> PMC10414525, </span><span class="title"> "Peer Review of &#8220;Representing Physician Suicide Claims as Nanopublications: Proof-of-Concept Study Creating Claim Networks&#8221;", </span><span><b> 3(3) </b></span><span> (2022): </span><span><i> JMIRx med, </i></span></li><li><span class="pmcid"> PMC10414225, </span><span class="author_string"> Chan E., </span><span class="title"> "Peer Review of &#8220;Representing Physician Suicide Claims as Nanopublications: Proof-of-Concept Study Creating Claim Networks&#8221;", </span><span><b> 3(3) </b></span><span> (2022): </span><span><i> JMIRx med, </i></span></li><li><span class="pmcid"> PMC11099147, </span><span class="author_string"> Ammar A, Evelo C, Willighagen E., </span><span class="title"> "FAIR assessment of nanosafety data reusability with community standards.", </span><span><b> 11(1) </b></span><span> (2024): </span><span><i> Scientific data, </i></span><span class="page_info">PI 503, </span><a href="https://doi.org/10.1038/s41597-024-03324-x"> DOI: 10.1038/s41597-024-03324-x, </a><span class="abstract_text" title="Nanomaterials hold great promise for improving our society, and it is crucial to understand their effects on biological systems in order to enhance their properties and ensure their safety. However, the lack of consistency in experimental reporting, the absence of universally accepted machine-readable metadata standards, and the challenge of combining such standards hamper the reusability of previously produced data for risk assessment. Fortunately, the research community has responded to these challenges by developing minimum reporting standards that address several of these issues. By converting twelve published minimum reporting standards into a machine-readable representation using FAIR maturity indicators, we have created a machine-friendly approach to annotate and assess datasets' reusability according to those standards. Furthermore, our NanoSafety Data Reusability Assessment (NSDRA) framework includes a metadata generator web application that can be integrated into experimental data management, and a new web application that can summarize the reusability of nanosafety datasets for one or more subsets of maturity indicators, tailored to specific computational risk assessment use cases. This approach enhances the transparency, communication, and reusability of experimental data and metadata. With this improved FAIR approach, we can facilitate the reuse of nanosafety research for exploration, toxicity prediction, and regulation, thereby advancing the field and benefiting society as a whole."> Abstract: 206 words, </span></li><li><span class="pmcid"> PMC11199958, </span><span class="author_string"> Akhmetova LA, Kurochkin AS, Frolov AV., </span><span class="title"> "Dung-beetles (Coleoptera, Scarabaeidae, Aphodiinae, Scarabaeinae) feeding on faeces of steppe marmots &lt;i&gt;Marmotabobak&lt;/i&gt; (Rodentia, Sciuridae) in Middle Volga territory.", </span><span><b> 12 </b></span><span> (2024): </span><span><i> Biodiversity data journal, </i></span><span class="page_info">PI e125090, </span><a href="https://doi.org/10.3897/bdj.12.e125090"> DOI: 10.3897/bdj.12.e125090, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;In open terrestrial biomes of Holarctic realm, ground squirrels are recognised as keystone species inhabiting steppes. They shape the plant species composition and diversity and support a fauna of species associated with their burrows. Ground squirrels and associated dung-beetles are important elements of the steppe food webs, yet the trophic associations between species are still poorly studied.&lt;h4&gt;New information&lt;/h4&gt;The area in the northern outskirts of Obshchy Syrt plateau, on the border of Samara and Orenburg Provinces of Russia was surveyed and scarab beetles (Scarabaeidae) feeding on steppe marmot (&lt;i&gt;Marmotabobak&lt;/i&gt; (M&#252;ller, 1776)) faeces were collected from six localities. Twenty eight species of two subfamilies - Aphodiinae and Scarabaeinae, - were identified with the majority of species belonging the genus &lt;i&gt;Aphodius&lt;/i&gt; Hellwig, 1798. Seven species are recorded as consumers of marmot faeces for the first time. Only two nidicolous specialist species were found which suggests that the studied population of steppe marmots is as result of the recent secondary colonisation and not all the associated scarab beetle faunas were re-established."> Abstract: 167 words, </span></li><li><span class="pmcid"> PMC7959648, </span><span class="author_string"> Kuhn T, Taelman R, Emonet V, Antonatos H, Soiland-Reyes S, Dumontier M., </span><span class="title"> "Semantic micro-contributions with decentralized nanopublication services.", </span><span><b> 7 </b></span><span> (2021): </span><span><i> PeerJ. Computer science, </i></span><span class="page_info">PI e387, </span><a href="https://doi.org/10.7717/peerj-cs.387"> DOI: 10.7717/peerj-cs.387, </a><span class="abstract_text" title="While the publication of Linked Data has become increasingly common, the process tends to be a relatively complicated and heavy-weight one. Linked Data is typically published by centralized entities in the form of larger dataset releases, which has the downside that there is a central bottleneck in the form of the organization or individual responsible for the releases. Moreover, certain kinds of data entries, in particular those with subjective or original content, currently do not fit into any existing dataset and are therefore more difficult to publish. To address these problems, we present here an approach to use nanopublications and a decentralized network of services to allow users to directly publish small Linked Data statements through a simple and user-friendly interface, called Nanobench, powered by semantic templates that are themselves published as nanopublications. The published nanopublications are cryptographically verifiable and can be queried through a redundant and decentralized network of services, based on the grlc API generator and a new quad extension of Triple Pattern Fragments. We show here that these two kinds of services are complementary and together allow us to query nanopublications in a reliable and efficient manner. We also show that Nanobench makes it indeed very easy for users to publish Linked Data statements, even for those who have no prior experience in Linked Data publishing."> Abstract: 219 words, </span></li><li><span class="pmcid"> PMC11131308, </span><span class="author_string"> Vogt L, Kuhn T, Hoehndorf R., </span><span class="title"> "Semantic units: organizing knowledge graphs into semantically meaningful units of representation.", </span><span><b> 15(1) </b></span><span> (2024): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 7, </span><a href="https://doi.org/10.1186/s13326-024-00310-5"> DOI: 10.1186/s13326-024-00310-5, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;In today's landscape of data management, the importance of knowledge graphs and ontologies is escalating as critical mechanisms aligned with the FAIR Guiding Principles-ensuring data and metadata are Findable, Accessible, Interoperable, and Reusable. We discuss three challenges that may hinder the effective exploitation of the full potential of FAIR knowledge graphs.&lt;h4&gt;Results&lt;/h4&gt;We introduce &quot;semantic units&quot; as a conceptual solution, although currently exemplified only in a limited prototype. Semantic units structure a knowledge graph into identifiable and semantically meaningful subgraphs by adding another layer of triples on top of the conventional data layer. Semantic units and their subgraphs are represented by their own resource that instantiates a corresponding semantic unit class. We distinguish statement and compound units as basic categories of semantic units. A statement unit is the smallest, independent proposition that is semantically meaningful for a human reader. Depending on the relation of its underlying proposition, it consists of one or more triples. Organizing a knowledge graph into statement units results in a partition of the graph, with each triple belonging to exactly one statement unit. A compound unit, on the other hand, is a semantically meaningful collection of statement and compound units that form larger subgraphs. Some semantic units organize the graph into different levels of representational granularity, others orthogonally into different types of granularity trees or different frames of reference, structuring and organizing the knowledge graph into partially overlapping, partially enclosed subgraphs, each of which can be referenced by its own resource.&lt;h4&gt;Conclusions&lt;/h4&gt;Semantic units, applicable in RDF/OWL and labeled property graphs, offer support for making statements about statements and facilitate graph-alignment, subgraph-matching, knowledge graph profiling, and for management of access restrictions to sensitive data. Additionally, we argue that organizing the graph into semantic units promotes the differentiation of ontological and discursive information, and that it also supports the differentiation of multiple frames of reference within the graph."> Abstract: 307 words, </span></li><li><span class="pmcid"> PMC9130601, </span><span class="author_string"> Schultes E, Roos M, Bonino da Silva Santos LO, Guizzardi G, Bouwman J, Hankemeier T, Baak A, Mons B., </span><span class="title"> "FAIR Digital Twins for Data-Intensive Research.", </span><span><b> 5 </b></span><span> (2022): </span><span><i> Frontiers in big data, </i></span><span class="page_info">PI 883341, </span><a href="https://doi.org/10.3389/fdata.2022.883341"> DOI: 10.3389/fdata.2022.883341, </a><span class="abstract_text" title="Although all the technical components supporting fully orchestrated Digital Twins (DT) currently exist, what remains missing is a conceptual clarification and analysis of a more generalized concept of a DT that is made FAIR, that is, universally machine actionable. This methodological overview is a first step toward this clarification. We present a review of previously developed semantic artifacts and how they may be used to compose a higher-order data model referred to here as a FAIR Digital Twin (FDT). We propose an architectural design to compose, store and reuse FDTs supporting data intensive research, with emphasis on privacy by design and their use in GDPR compliant open science."> Abstract: 108 words, </span></li><li><span class="pmcid"> PMC7959622, </span><span class="author_string"> Giachelle F, Dosso D, Silvello G., </span><span class="title"> "Search, access, and explore life science nanopublications on the Web.", </span><span><b> 7 </b></span><span> (2021): </span><span><i> PeerJ. Computer science, </i></span><span class="page_info">PI e335, </span><a href="https://doi.org/10.7717/peerj-cs.335"> DOI: 10.7717/peerj-cs.335, </a><span class="abstract_text" title="Nanopublications are Resource Description Framework (RDF) graphs encoding scientific facts extracted from the literature and enriched with provenance and attribution information. There are millions of nanopublications currently available on the Web, especially in the life science domain. Nanopublications are thought to facilitate the discovery, exploration, and re-use of scientific facts. Nevertheless, they are still not widely used by scientists outside specific circles; they are hard to find and rarely cited. We believe this is due to the lack of services to seek, find and understand nanopublications' content. To this end, we present the NanoWeb application to seamlessly search, access, explore, and re-use the nanopublications publicly available on the Web. For the time being, NanoWeb focuses on the life science domain where the vastest amount of nanopublications are available. It is a unified access point to the world of nanopublications enabling search over graph data, direct connections to evidence papers, and scientific curated databases, and visual and intuitive exploration of the relation network created by the encoded scientific facts."> Abstract: 168 words, </span></li><li><span class="pmcid"> PMC8096966, </span><span class="author_string"> Ehrhart F, Willighagen EL, Kutmon M, van Hoften M, Curfs LMG, Evelo CT., </span><span class="title"> "A resource to explore the discovery of rare diseases and their causative genes.", </span><span><b> 8(1) </b></span><span> (2021): </span><span><i> Scientific data, </i></span><span class="page_info">PI 124, </span><a href="https://doi.org/10.1038/s41597-021-00905-y"> DOI: 10.1038/s41597-021-00905-y, </a><span class="abstract_text" title="Here, we describe a dataset with information about monogenic, rare diseases with a known genetic background, supplemented with manually extracted provenance for the disease itself and the discovery of the underlying genetic cause. We assembled a collection of 4166 rare monogenic diseases and linked them to 3163 causative genes, annotated with OMIM and Ensembl identifiers and HGNC symbols. The PubMed identifiers of the scientific publications, which for the first time described the rare diseases, and the publications, which found the genes causing the diseases were added using information from OMIM, PubMed, Wikipedia, whonamedit.com, and Google Scholar. The data are available under CC0 license as spreadsheet and as RDF in a semantic model modified from DisGeNET, and was added to Wikidata. This dataset relies on publicly available data and publications with a PubMed identifier, but by our effort to make the data interoperable and linked, we can now analyse this data. Our analysis revealed the timeline of rare disease and causative gene discovery and links them to developments in methods."> Abstract: 169 words, </span></li><li><span class="pmcid"> PMC8486731, </span><span class="author_string"> Dimitrova M, Senderov VE, Georgiev T, Zhelezov G, Penev L., </span><span class="title"> "Infrastructure and Population of the OpenBiodiv Biodiversity Knowledge Graph.", </span><span><b> 9 </b></span><span> (2021): </span><span><i> Biodiversity data journal, </i></span><span class="page_info">PI e67671, </span><a href="https://doi.org/10.3897/bdj.9.e67671"> DOI: 10.3897/bdj.9.e67671, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;OpenBiodiv is a biodiversity knowledge graph containing a synthetic linked open dataset, OpenBiodiv-LOD, which combines knowledge extracted from academic literature with the taxonomic backbone used by the Global Biodiversity Information Facility. The linked open data is modelled according to the OpenBiodiv-O ontology integrating semantic resource types from recognised biodiversity and publishing ontologies with OpenBiodiv-O resource types, introduced to capture the semantics of resources not modelled before.&lt;h4&gt;New information&lt;/h4&gt;We introduce the new release of the OpenBiodiv-LOD attained through information extraction and modelling of additional biodiversity entities. It was achieved by further developments to OpenBiodiv-O, the data storage infrastructure and the workflow and accompanying R software packages used for transformation of academic literature into Resource Description Framework (RDF). We discuss how to utilise the LOD in biodiversity informatics and give examples by providing solutions to several competency questions. We investigate performance issues that arise due to the large amount of inferred statements in the graph and conclude that OWL-full inference is impractical for the project and that unnecessary inference should be avoided."> Abstract: 169 words, </span></li><li><span class="pmcid"> PMC7779061, </span><span class="author_string"> Martens M, Ammar A, Riutta A, Waagmeester A, Slenter DN, Hanspers K, A Miller R, Digles D, Lopes EN, Ehrhart F, Dupuis LJ, Winckers LA, Coort SL, Willighagen EL, Evelo CT, Pico AR, Kutmon M., </span><span class="title"> "WikiPathways: connecting communities.", </span><span><b> 49(D1) </b></span><span> (2021): </span><span><i> Nucleic acids research, </i></span><span class="page_info">PI D613-D621, </span><a href="https://doi.org/10.1093/nar/gkaa1024"> DOI: 10.1093/nar/gkaa1024, </a><span class="abstract_text" title="WikiPathways (https://www.wikipathways.org) is a biological pathway database known for its collaborative nature and open science approaches. With the core idea of the scientific community developing and curating biological knowledge in pathway models, WikiPathways lowers all barriers for accessing and using its content. Increasingly more content creators, initiatives, projects and tools have started using WikiPathways. Central in this growth and increased use of WikiPathways are the various communities that focus on particular subsets of molecular pathways such as for rare diseases and lipid metabolism. Knowledge from published pathway figures helps prioritize pathway development, using optical character and named entity recognition. We show the growth of WikiPathways over the last three years, highlight the new communities and collaborations of pathway authors and curators, and describe various technologies to connect to external resources and initiatives. The road toward a sustainable, community-driven pathway database goes through integration with other resources such as Wikidata and allowing more use, curation and redistribution of WikiPathways content."> Abstract: 159 words, </span></li><li><span class="pmcid"> PMC9897605, </span><span class="author_string"> Willighagen E., </span><span class="title"> "Two years of explicit CiTO annotations.", </span><span><b> 15(1) </b></span><span> (2023): </span><span><i> Journal of cheminformatics, </i></span><span class="page_info">PI 14, </span><a href="https://doi.org/10.1186/s13321-023-00683-2"> DOI: 10.1186/s13321-023-00683-2, </a><span class="abstract_text" title="Citations are an essential aspect of research communication and have become the basis of many evaluation metrics in the academic world. Some see citation counts as a mark of scientific impact or even quality, but in reality the reasons for citing other work are manifold which makes the interpretation more complicated than a single citation count can reflect. Two years ago, the Journal of Cheminformatics proposed the CiTO Pilot for the adoption of a practice of annotating citations with their citation intentions. Basically, when you cite a journal article or dataset (or any other source), you also explain why specifically you cite that source. Particularly, the agreement and disagreement and reuse of methods and data are of interest. This article explores what happened after the launch of the pilot. We summarize how authors in the Journal of Cheminformatics used the pilot, shows citation annotations are distributed with Wikidata, visualized with Scholia, discusses adoption outside BMC, and finally present some thoughts on what needs to happen next."> Abstract: 166 words, </span></li><li><span class="pmcid"> PMC10880534, </span><span class="author_string"> Zhang X, Zheng H, Zeng Y, Zou J, Zhao L., </span><span class="title"> "Exploring how health-related advertising interference contributes to the development of cyberchondria: A stressor-strain-outcome approach.", </span><span><b> 10 </b></span><span> (2024): </span><span><i> Digital health, </i></span><span class="page_info">PI 20552076241233138, </span><a href="https://doi.org/10.1177/20552076241233138"> DOI: 10.1177/20552076241233138, </a><span class="abstract_text" title="&lt;h4&gt;Objectives&lt;/h4&gt;Cyberchondria is increasingly recognized as the dark side of digital health, given the pervasive use of the internet as a main source of health information in people's daily lives. While previous studies have identified many factors contributing to cyberchondria, there is a dearth of research on the impact of health-related advertisements. Therefore, this study adopts the stressor-strain-outcome (SSO) model to investigate how health-related advertising interference is directly and indirectly related to cyberchondria.&lt;h4&gt;Methods&lt;/h4&gt;To empirically validate the proposed research model, we conducted an online survey with 437 internet users with medical information seeking experience in China. Structural equation modeling (SEM) was employed to analyze the survey data.&lt;h4&gt;Results&lt;/h4&gt;Our findings revealed a positive, direct association between health-related advertising interference and cyberchondria. Meanwhile, advertising interference was positively related to both information overload and information irrelevance, with the former further predicting cyberchondria. Moreover, doctor-patient communication weakened the positive effect of information overload on cyberchondria.&lt;h4&gt;Conclusions&lt;/h4&gt;The study not only theoretically contributes to the literature by theorizing the relationship between health-related advertising interference and cyberchondria but also practically underlines the pivotal role of effective doctor-patient communication in reducing the development of cyberchondria."> Abstract: 183 words, </span></li><li><span class="pmcid"> PMC10353186, </span><span class="author_string"> Seneviratne O, Das AK, Chari S, Agu NN, Rashid SM, McCusker J, Franklin JS, Qi M, Bennett KP, Chen CH, Hendler JA, McGuinness DL., </span><span class="title"> "Semantically enabling clinical decision support recommendations.", </span><span><b> 14(1) </b></span><span> (2023): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 8, </span><a href="https://doi.org/10.1186/s13326-023-00285-9"> DOI: 10.1186/s13326-023-00285-9, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Clinical decision support systems have been widely deployed to guide healthcare decisions on patient diagnosis, treatment choices, and patient management through evidence-based recommendations. These recommendations are typically derived from clinical practice guidelines created by clinical specialties or healthcare organizations. Although there have been many different technical approaches to encoding guideline recommendations into decision support systems, much of the previous work has not focused on enabling system generated recommendations through the formalization of changes in a guideline, the provenance of a recommendation, and applicability of the evidence. Prior work indicates that healthcare providers may not find that guideline-derived recommendations always meet their needs for reasons such as lack of relevance, transparency, time pressure, and applicability to their clinical practice.&lt;h4&gt;Results&lt;/h4&gt;We introduce several semantic techniques that model diseases based on clinical practice guidelines, provenance of the guidelines, and the study cohorts they are based on to enhance the capabilities of clinical decision support systems. We have explored ways to enable clinical decision support systems with semantic technologies that can represent and link to details in related items from the scientific literature and quickly adapt to changing information from the guidelines, identifying gaps, and supporting personalized explanations. Previous semantics-driven clinical decision systems have limited support in all these aspects, and we present the ontologies and semantic web based software tools in three distinct areas that are unified using a standard set of ontologies and a custom-built knowledge graph framework: (i) guideline modeling to characterize diseases, (ii) guideline provenance to attach evidence to treatment decisions from authoritative sources, and (iii) study cohort modeling to identify relevant research publications for complicated patients.&lt;h4&gt;Conclusions&lt;/h4&gt;We have enhanced existing, evidence-based knowledge by developing ontologies and software that enables clinicians to conveniently access updates to and provenance of guidelines, as well as gather additional information from research studies applicable to their patients' unique circumstances. Our software solutions leverage many well-used existing biomedical ontologies and build upon decades of knowledge representation and reasoning work, leading to explainable results."> Abstract: 326 words, </span></li><li><span class="pmcid"> PMC7746500, </span><span class="author_string"> Massey PM, Kearney MD, Hauer MK, Selvan P, Koku E, Leader AE., </span><span class="title"> "Dimensions of Misinformation About the HPV Vaccine on Instagram: Content and Network Analysis of Social Media Characteristics.", </span><span><b> 22(12) </b></span><span> (2020): </span><span><i> Journal of medical Internet research, </i></span><span class="page_info">PI e21451, </span><a href="https://doi.org/10.2196/21451"> DOI: 10.2196/21451, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;The human papillomavirus (HPV) vaccine is a major advancement in cancer prevention and this primary prevention tool has the potential to reduce and eliminate HPV-associated cancers; however, the safety and efficacy of vaccines in general and the HPV vaccine specifically have come under attack, particularly through the spread of misinformation on social media. The popular social media platform Instagram represents a significant source of exposure to health (mis)information; 1 in 3 US adults use Instagram.&lt;h4&gt;Objective&lt;/h4&gt;The objective of this analysis was to characterize pro- and anti-HPV vaccine networks on Instagram, and to describe misinformation within the anti-HPV vaccine network.&lt;h4&gt;Methods&lt;/h4&gt;From April 2018 to December 2018, we collected publicly available English-language Instagram posts containing hashtags #HPV, #HPVVaccine, or #Gardasil using Netlytic software (n=16,607). We randomly selected 10% of the sample and content analyzed relevant posts (n=580) for text, image, and social media features as well as holistic attributes (eg, sentiments, personal stories). Among antivaccine posts, we organized elements of misinformation within four broad dimensions: 1) misinformation theoretical domains, 2) vaccine debate topics, 3) evidence base, and 4) health beliefs. We conducted univariate, bivariate, and network analyses on the subsample of posts to quantify the role and position of individual posts in the network.&lt;h4&gt;Results&lt;/h4&gt;Compared to provaccine posts (324/580, 55.9%), antivaccine posts (256/580, 44.1%) were more likely to originate from individuals (64.1% antivaccine vs 25.0% provaccine; P&lt;.001) and include personal narratives (37.1% vs 25.6%; P=.003). In the antivaccine network, core misinformation characteristics included mentioning #Gardasil, purporting to reveal a lie (ie, concealment), conspiracy theories, unsubstantiated claims, and risk of vaccine injury. Information/resource posts clustered around misinformation domains including falsification, nanopublications, and vaccine-preventable disease, whereas personal narrative posts clustered around different domains of misinformation, including concealment, injury, and conspiracy theories. The most liked post (6634 likes) in our full subsample was a positive personal narrative post, created by a non-health individual; the most liked post (5604 likes) in our antivaccine subsample was an informational post created by a health individual.&lt;h4&gt;Conclusions&lt;/h4&gt;Identifying characteristics of misinformation related to HPV vaccine on social media will inform targeted interventions (eg, network opinion leaders) and help sow corrective information and stories tailored to different falsehoods."> Abstract: 353 words, </span></li><li><span class="pmcid"> PMC6066477, </span><span class="author_string"> Page R., </span><span class="title"> "Liberating links between datasets using lightweight data publishing: an example using plant names and the taxonomic literature.", </span><span> (2018): </span><span><i> Biodiversity data journal, </i></span><span class="page_info">PI e27539, </span><a href="https://doi.org/10.3897/bdj.6.e27539"> DOI: 10.3897/bdj.6.e27539, </a><span class="abstract_text" title='Constructing a biodiversity knowledge graph will require making millions of cross links between diversity entities in different datasets. Researchers trying to bootstrap the growth of the biodiversity knowledge graph by constructing databases of links between these entities lack obvious ways to publish these sets of links. One appealing and lightweight approach is to create a "datasette", a database that is wrapped together with a simple web server that enables users to query the data. Datasettes can be packaged into Docker containers and hosted online with minimal effort. This approach is illustrated using a dataset of links between globally unique identifiers for plant taxonomic namesand identifiers for the taxonomic articles that published those names.'> Abstract: 113 words, </span></li><li><span class="pmcid"> PMC9361271, </span><span class="author_string"> Verma S, Bhatia R, Harit S, Batish S., </span><span class="title"> "Scholarly knowledge graphs through structuring scholarly communication: a review.", </span><span><b> 9(1) </b></span><span> (2023): </span><span><i> Complex &amp; intelligent systems, </i></span><span class="page_info">PI 1059-1095, </span><a href="https://doi.org/10.1007/s40747-022-00806-6"> DOI: 10.1007/s40747-022-00806-6, </a><span class="abstract_text" title="The necessity for scholarly knowledge mining and management has grown significantly as academic literature and its linkages to authors produce enormously. Information extraction, ontology matching, and accessing academic components with relations have become more critical than ever. Therefore, with the advancement of scientific literature, scholarly knowledge graphs have become critical to various applications where semantics can impart meanings to concepts. The objective of study is to report a literature review regarding knowledge graph construction, refinement and utilization in scholarly domain. Based on scholarly literature, the study presents a complete assessment of current state-of-the-art techniques. We presented an analytical methodology to investigate the existing status of &lt;i&gt;scholarly knowledge graphs&lt;/i&gt; (SKG) by structuring scholarly communication. This review paper investigates the field of applying machine learning, rule-based learning, and natural language processing tools and approaches to construct SKG. It further presents the review of knowledge graph utilization and refinement to provide a view of current research efforts. In addition, we offer existing applications and challenges across the board in construction, refinement and utilization collectively. This research will help to identify frontier trends of SKG which will motivate future researchers to carry forward their work."> Abstract: 191 words, </span></li><li><span class="pmcid"> PMC8579612, </span><span class="author_string"> Amith M, Onye C, Ledoux T, Xiong G, Tao C., </span><span class="title"> "The ontology of fast food facts: conceptualization of nutritional fast food data for consumers and semantic web applications.", </span><span><b> 21(Suppl 7) </b></span><span> (2021): </span><span><i> BMC medical informatics and decision making, </i></span><span class="page_info">PI 275, </span><a href="https://doi.org/10.1186/s12911-021-01636-1"> DOI: 10.1186/s12911-021-01636-1, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Fast food with its abundance and availability to consumers may have health consequences due to the high calorie intake which is a major contributor to life threatening diseases. Providing nutritional information has some impact on consumer decisions to self regulate and promote healthier diets, and thus, government regulations have mandated the publishing of nutritional content to assist consumers, including for fast food. However, fast food nutritional information is fragmented, and we realize a benefit to collate nutritional data to synthesize knowledge for individuals.&lt;h4&gt;Methods&lt;/h4&gt;We developed the ontology of fast food facts as an opportunity to standardize knowledge of fast food and link nutritional data that could be analyzed and aggregated for the information needs of consumers and experts. The ontology is based on metadata from 21 fast food establishment nutritional resources and authored in OWL2 using Prot&#233;g&#233;.&lt;h4&gt;Results&lt;/h4&gt;Three evaluators reviewed the logical structure of the ontology through natural language translation of the axioms. While there is majority agreement (76.1% pairwise agreement) of the veracity of the ontology, we identified 103 out of the 430 statements that were erroneous. We revised the ontology and publicably published the initial release of the ontology. The ontology has 413 classes, 21 object properties, 13 data properties, and 494 logical axioms.&lt;h4&gt;Conclusion&lt;/h4&gt;With the initial release of the ontology of fast food facts we discuss some future visions with the continued evolution of this knowledge base, and the challenges we plan to address, like the management and publication of voluminous amount of semantically linked fast food nutritional data."> Abstract: 249 words, </span></li><li><span class="pmcid"> PMC4603842, </span><span class="author_string"> Mina E, Thompson M, Kaliyaperumal R, Zhao J, der Horst vE, Tatum Z, Hettne KM, Schultes EA, Mons B, Roos M., </span><span class="title"> "Nanopublications for exposing experimental data in the life-sciences: a Huntington's Disease case study.", </span><span><b> 6 </b></span><span> (2015): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 5, </span><a href="https://doi.org/10.1186/2041-1480-6-5"> DOI: 10.1186/2041-1480-6-5, </a><span class="abstract_text" title="Data from high throughput experiments often produce far more results than can ever appear in the main text or tables of a single research article. In these cases, the majority of new associations are often archived either as supplemental information in an arbitrary format or in publisher-independent databases that can be difficult to find. These data are not only lost from scientific discourse, but are also elusive to automated search, retrieval and processing. Here, we use the nanopublication model to make scientific assertions that were concluded from a workflow analysis of Huntington's Disease data machine-readable, interoperable, and citable. We followed the nanopublication guidelines to semantically model our assertions as well as their provenance metadata and authorship. We demonstrate interoperability by linking nanopublication provenance to the Research Object model. These results indicate that nanopublications can provide an incentive for researchers to expose data that is interoperable and machine-readable for future use and preservation for which they can get credits for their effort. Nanopublications can have a leading role into hypotheses generation offering opportunities to produce large-scale data integration."> Abstract: 177 words, </span></li><li><span class="pmcid"> PMC9205953, </span><span class="author_string"> Blagec K, Barbosa-Silva A, Ott S, Samwald M., </span><span class="title"> "A curated, ontology-based, large-scale knowledge graph of artificial intelligence tasks and benchmarks.", </span><span><b> 9(1) </b></span><span> (2022): </span><span><i> Scientific data, </i></span><span class="page_info">PI 322, </span><a href="https://doi.org/10.1038/s41597-022-01435-x"> DOI: 10.1038/s41597-022-01435-x, </a><span class="abstract_text" title="Research in artificial intelligence (AI) is addressing a growing number of tasks through a rapidly growing number of models and methodologies. This makes it difficult to keep track of where novel AI methods are successfully - or still unsuccessfully - applied, how progress is measured, how different advances might synergize with each other, and how future research should be prioritized. To help address these issues, we created the Intelligence Task Ontology and Knowledge Graph (ITO), a comprehensive, richly structured and manually curated resource on artificial intelligence tasks, benchmark results and performance metrics. The current version of ITO contains 685,560 edges, 1,100 classes representing AI processes and 1,995 properties representing performance metrics. The primary goal of ITO is to enable analyses of the global landscape of AI tasks and capabilities. ITO is based on technologies that allow for easy integration and enrichment with external data, automated inference and continuous, collaborative expert curation of underlying ontological models. We make the ITO dataset and a collection of Jupyter notebooks utilizing ITO openly available."> Abstract: 170 words, </span></li><li><span class="pmcid"> PMC4582753, </span><span class="author_string"> Verspoor K, Kim J, Dumontier M., </span><span class="title"> "Interoperability of text corpus annotations with the semantic web", </span><span><b> 9(Suppl 5) </b></span><span> (2015): </span><span><i> BMC proceedings, </i></span><span class="page_info">PI A2-A2, </span><a href="https://doi.org/10.1186/1753-6561-9-s5-a2"> DOI: 10.1186/1753-6561-9-s5-a2, </a></li><li><span class="pmcid"> PMC6119244, </span><span class="author_string"> Amith M, Tao C., </span><span class="title"> "Representing vaccine misinformation using ontologies.", </span><span><b> 9(1) </b></span><span> (2018): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 22, </span><a href="https://doi.org/10.1186/s13326-018-0190-0"> DOI: 10.1186/s13326-018-0190-0, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;In this paper, we discuss the design and development of a formal ontology to describe misinformation about vaccines. Vaccine misinformation is one of the drivers leading to vaccine hesitancy in patients. While there are various levels of vaccine hesitancy to combat and specific interventions to address those levels, it is important to have tools that help researchers understand this problem. With an ontology, not only can we collect and analyze varied misunderstandings about vaccines, but we can also develop tools that can provide informatics solutions.&lt;h4&gt;Results&lt;/h4&gt;We developed the Vaccine Misinformation Ontology (VAXMO) that extends the Misinformation Ontology and links to the nanopublication Resource Description Framework (RDF) model for false assertions of vaccines. Preliminary assessment using semiotic evaluation metrics indicated adequate quality for our ontology. We outlined and demonstrated proposed uses of the ontology to detect and understand anti-vaccine information.&lt;h4&gt;Conclusion&lt;/h4&gt;We surmised that VAXMO and its proposed use cases can support tools and technology that can pave the way for vaccine misinformation detection and analysis. Using an ontology, we can formally structure knowledge for machines and software to better understand the vaccine misinformation domain."> Abstract: 181 words, </span></li><li><span class="pmcid"> PMC4769089, </span><span class="author_string"> Hettne KM, Thompson M, van Haagen HH, van der Horst E, Kaliyaperumal R, Mina E, Tatum Z, Laros JF, van Mulligen EM, Schuemie M, Aten E, Li TS, Bruskiewich R, Good BM, Su AI, Kors JA, den Dunnen J, van Ommen GJ, Roos M, 't Hoen PA, Mons B, Schultes EA., </span><span class="title"> "The Implicitome: A Resource for Rationalizing Gene-Disease Associations.", </span><span><b> 11(2) </b></span><span> (2016): </span><span><i> PloS one, </i></span><span class="page_info">PI e0149621, </span><a href="https://doi.org/10.1371/journal.pone.0149621"> DOI: 10.1371/journal.pone.0149621, </a><span class="abstract_text" title="High-throughput experimental methods such as medical sequencing and genome-wide association studies (GWAS) identify increasingly large numbers of potential relations between genetic variants and diseases. Both biological complexity (millions of potential gene-disease associations) and the accelerating rate of data production necessitate computational approaches to prioritize and rationalize potential gene-disease relations. Here, we use concept profile technology to expose from the biomedical literature both explicitly stated gene-disease relations (the explicitome) and a much larger set of implied gene-disease associations (the implicitome). Implicit relations are largely unknown to, or are even unintended by the original authors, but they vastly extend the reach of existing biomedical knowledge for identification and interpretation of gene-disease associations. The implicitome can be used in conjunction with experimental data resources to rationalize both known and novel associations. We demonstrate the usefulness of the implicitome by rationalizing known and novel gene-disease associations, including those from GWAS. To facilitate the re-use of implicit gene-disease associations, we publish our data in compliance with FAIR Data Publishing recommendations [https://www.force11.org/group/fairgroup] using nanopublications. An online tool (http://knowledge.bio) is available to explore established and potential gene-disease associations in the context of other biomedical relations."> Abstract: 188 words, </span></li><li><span class="pmcid"> PMC6754447, </span><span class="author_string"> Wilkinson MD, Dumontier M, Sansone SA, Bonino da Silva Santos LO, Prieto M, Batista D, McQuilton P, Kuhn T, Rocca-Serra P, Crosas M, Schultes E., </span><span class="title"> "Evaluating FAIR maturity through a scalable, automated, community-governed framework.", </span><span><b> 6(1) </b></span><span> (2019): </span><span><i> Scientific data, </i></span><span class="page_info">PI 174, </span><a href="https://doi.org/10.1038/s41597-019-0184-5"> DOI: 10.1038/s41597-019-0184-5, </a><span class="abstract_text" title='Transparent evaluations of FAIRness are increasingly required by a wide range of stakeholders, from scientists to publishers, funding agencies and policy makers. We propose a scalable, automatable framework to evaluate digital resources that encompasses measurable indicators, open source tools, and participation guidelines, which come together to accommodate domain relevant community-defined FAIR assessments. The components of the framework are: (1) Maturity Indicators - community-authored specifications that delimit a specific automatically-measurable FAIR behavior; (2) Compliance Tests - small Web apps that test digital resources against individual Maturity Indicators; and (3) the Evaluator, a Web application that registers, assembles, and applies community-relevant sets of Compliance Tests against a digital resource, and provides a detailed report about what a machine "sees" when it visits that resource. We discuss the technical and social considerations of FAIR assessments, and how this translates to our community-driven infrastructure. We then illustrate how the output of the Evaluator tool can serve as a roadmap to assist data stewards to incrementally and realistically improve the FAIRness of their resources.'> Abstract: 169 words, </span></li><li><span class="pmcid"> PMC4310165, </span><span class="author_string"> Lizio M, Harshbarger J, Shimoji H, Severin J, Kasukawa T, Sahin S, Abugessaisa I, Fukuda S, Hori F, Ishikawa-Kato S, Mungall CJ, Arner E, Baillie JK, Bertin N, Bono H, de Hoon M, Diehl AD, Dimont E, Freeman TC, Fujieda K, Hide W, Kaliyaperumal R, Katayama T, Lassmann T, Meehan TF, Nishikata K, Ono H, Rehli M, Sandelin A, Schultes EA, 't Hoen PA, Tatum Z, Thompson M, Toyoda T, Wright DW, Daub CO, Itoh M, Carninci P, Hayashizaki Y, Forrest AR, Kawaji H, FANTOM consortium., </span><span class="title"> "Gateways to the FANTOM5 promoter level mammalian expression atlas.", </span><span><b> 16 </b></span><span> (2015): </span><span><i> Genome biology, </i></span><span class="page_info">PI 22, </span><a href="https://doi.org/10.1186/s13059-014-0560-6"> DOI: 10.1186/s13059-014-0560-6, </a><span class="abstract_text" title="The FANTOM5 project investigates transcription initiation activities in more than 1,000 human and mouse primary cells, cell lines and tissues using CAGE. Based on manual curation of sample information and development of an ontology for sample classification, we assemble the resulting data into a centralized data resource (http://fantom.gsc.riken.jp/5/). This resource contains web-based tools and data-access points for the research community to search and extract data related to samples, genes, promoter activities, transcription factors and enhancers across the FANTOM5 atlas."> Abstract: 79 words, </span></li><li><span class="pmcid"> PMC5887437, </span><span class="author_string"> Ganapathiraju M, Balakrishnan N., </span><span class="title"> "T188. MASSIVE OPEN ONLINE DISCOVERY &#8216;MOOD&#8217; FOR SCHIZOPHRENIA RESEARCH", </span><span><b> 44(Suppl 1) </b></span><span> (2018): </span><span><i> Schizophrenia bulletin, </i></span><span class="page_info">PI S189-S189, </span><span class="abstract_text" title="Abstract &lt;h4&gt;Background&lt;/h4&gt; We have recently presented Schizophrenia Interactome, i.e., the network of protein-protein interactions (PPIs) of schizophrenia associated genes. PPIs predicted by our High-precision Protein-Protein Interaction Prediction model (HiPPIP) which uses machine learning to classify features of protein-pairs such as colocalization, coexpression, common molecular functions and biological processes could explain the apparent discordance between modern and historical genetic basis of Schizophrenia (published in npj Schizophrenia), and also were instrumental in discovering that OASL interacts DDX58 to activate the RIG-I immunity pathway during viral infection. These novel predicted PPIs were found to be highly accurate based on computational and experimental validations, and gave insights into possible functions of SZ genes that previously had no-known functional information. Even a single novel PPI can have enormous impact on advancement of biology, when translated effectively. How can we ensure that 500+ of these novel PPIs of schizophrenia interactome are translated effectively? &lt;h4&gt;Methods&lt;/h4&gt; We developed a platform for Massive Open Online Discovery for Schizophrenia Research, or &#8220;MOOD for Schizophrenia Research&#8221;, that brings together trained biologists and bioinformaticians including those who are currently not affiliated with research labs (non-research students, PhDs who gave up science careers for administrative/corporate jobs or to take care of families, etc), as well as scientists who are active researchers, to hypothesize, discuss and prioritize the novel PPIs. Hypotheses are written as nanopublications with authorship credit. We are developing a number of features on the portal that allow and encourage scientists to create knowledge around the predicted PPIs and be recognized and given credit. &lt;h4&gt;Results&lt;/h4&gt; The first version of our website that disseminates the Schizophrenia Interactome is receiving hundreds of unique users each month. We have since developed MOOD for Schizophrenia Research and will present the key features of the website in this work. Each PPI can be viewed, researched on and written about, by participating scientists. Comprehensive information about the proteins in the PPI regarding their known functions, pathways, diseases and drug associations, is made readily available to scientists, allowing them to hypothesize the importance of the specific PPI. We present methods that we employ to promote collaboration in this work. Initially, the portal starts with a few members and it grows through referral webs (i.e. current members invite new members). The portal has a number of features that recognize and thus entice users to participate. We will also present the user feedback and participation. &lt;h4&gt;Discussion&lt;/h4&gt; PPIs are central to cellular systems. Yet less than 10% of estimated PPIs are known today. Thus, the computationally predicted PPIs which are deemed accurate, can accelerate advancement of schizophrenia biology research. The time is ripe to benefit from computer science and information technologies methods for not only discovering aspects of computational biology but to create new mechanisms to promote online collaboration to achieve big things as a summation of nanocontributions. The knowledge potential generated through this system would aid various principal investigators in well established (but ill funded) research labs by giving them access to bioinformaticians and biologists around the world who are eager be recognized for their contributions. We present here, not merely a website but a novel approach to promote collaborative research between people with heterogeneous skills and commitments by benefiting from the untapped talent of researchers around the world."> Abstract: 538 words, </span></li><li><span class="pmcid"> PMC4495984, </span><span class="author_string"> Gonz&#225;lez-Beltr&#225;n A, Li P, Zhao J, Avila-Garcia MS, Roos M, Thompson M, van der Horst E, Kaliyaperumal R, Luo R, Lee TL, Lam TW, Edmunds SC, Sansone SA, Rocca-Serra P., </span><span class="title"> "From Peer-Reviewed to Peer-Reproduced in Scholarly Publishing: The Complementary Roles of Data Models and Workflows in Bioinformatics.", </span><span><b> 10(7) </b></span><span> (2015): </span><span><i> PloS one, </i></span><span class="page_info">PI e0127612, </span><a href="https://doi.org/10.1371/journal.pone.0127612"> DOI: 10.1371/journal.pone.0127612, </a><span class="abstract_text" title="&lt;h4&gt;Motivation&lt;/h4&gt;Reproducing the results from a scientific paper can be challenging due to the absence of data and the computational tools required for their analysis. In addition, details relating to the procedures used to obtain the published results can be difficult to discern due to the use of natural language when reporting how experiments have been performed. The Investigation/Study/Assay (ISA), Nanopublications (NP), and Research Objects (RO) models are conceptual data modelling frameworks that can structure such information from scientific papers. Computational workflow platforms can also be used to reproduce analyses of data in a principled manner. We assessed the extent by which ISA, NP, and RO models, together with the Galaxy workflow system, can capture the experimental processes and reproduce the findings of a previously published paper reporting on the development of SOAPdenovo2, a de novo genome assembler.&lt;h4&gt;Results&lt;/h4&gt;Executable workflows were developed using Galaxy, which reproduced results that were consistent with the published findings. A structured representation of the information in the SOAPdenovo2 paper was produced by combining the use of ISA, NP, and RO models. By structuring the information in the published paper using these data and scientific workflow modelling frameworks, it was possible to explicitly declare elements of experimental design, variables, and findings. The models served as guides in the curation of scientific information and this led to the identification of inconsistencies in the original published paper, thereby allowing its authors to publish corrections in the form of an errata.&lt;h4&gt;Availability&lt;/h4&gt;SOAPdenovo2 scripts, data, and results are available through the GigaScience Database: http://dx.doi.org/10.5524/100044; the workflows are available from GigaGalaxy: http://galaxy.cbiit.cuhk.edu.hk; and the representations using the ISA, NP, and RO models are available through the SOAPdenovo2 case study website http://isa-tools.github.io/soapdenovo2/.&lt;h4&gt;Contact&lt;/h4&gt;philippe.rocca-serra@oerc.ox.ac.uk and susanna-assunta.sansone@oerc.ox.ac.uk."> Abstract: 279 words, </span></li><li><span class="pmcid"> PMC10151034, </span><span class="author_string"> McCusker JP, Dumontier M, Yan R, He S, Dordick JS, McGuinness DL., </span><span class="title"> "Finding melanoma drugs through a probabilistic knowledge graph.", </span><span><b> 3 </b></span><span> (2017): </span><span><i> PeerJ. Computer science, </i></span><span class="page_info">PI e106, </span><a href="https://doi.org/10.7717/peerj-cs.106"> DOI: 10.7717/peerj-cs.106, </a><span class="abstract_text" title="Metastatic cutaneous melanoma is an aggressive skin cancer with some progression-slowing treatments but no known cure. The omics data explosion has created many possible drug candidates; however, filtering criteria remain challenging, and systems biology approaches have become fragmented with many disconnected databases. Using drug, protein and disease interactions, we built an evidence-weighted knowledge graph of integrated interactions. Our knowledge graph-based system, ReDrugS, can be used via an application programming interface or web interface, and has generated 25 high-quality melanoma drug candidates. We show that probabilistic analysis of systems biology graphs increases drug candidate quality compared to non-probabilistic methods. Four of the 25 candidates are novel therapies, three of which have been tested with other cancers. All other candidates have current or completed clinical trials, or have been studied in in vivo or in vitro. This approach can be used to identify candidate therapies for use in research or personalized medicine."> Abstract: 150 words, </span></li><li><span class="pmcid"> PMC3579732, </span><span class="author_string"> Beck T, Free RC, Thorisson GA, Brookes AJ., </span><span class="title"> "Semantically enabling a genome-wide association study database.", </span><span><b> 3(1) </b></span><span> (2012): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 9, </span><a href="https://doi.org/10.1186/2041-1480-3-9"> DOI: 10.1186/2041-1480-3-9, </a><span class="abstract_text" title="&lt;h4&gt;Unlabelled&lt;/h4&gt;&lt;h4&gt;Background&lt;/h4&gt;The amount of data generated from genome-wide association studies (GWAS) has grown rapidly, but considerations for GWAS phenotype data reuse and interchange have not kept pace. This impacts on the work of GWAS Central - a free and open access resource for the advanced querying and comparison of summary-level genetic association data. The benefits of employing ontologies for standardising and structuring data are widely accepted. The complex spectrum of observed human phenotypes (and traits), and the requirement for cross-species phenotype comparisons, calls for reflection on the most appropriate solution for the organisation of human phenotype data. The Semantic Web provides standards for the possibility of further integration of GWAS data and the ability to contribute to the web of Linked Data.&lt;h4&gt;Results&lt;/h4&gt;A pragmatic consideration when applying phenotype ontologies to GWAS data is the ability to retrieve all data, at the most granular level possible, from querying a single ontology graph. We found the Medical Subject Headings (MeSH) terminology suitable for describing all traits (diseases and medical signs and symptoms) at various levels of granularity and the Human Phenotype Ontology (HPO) most suitable for describing phenotypic abnormalities (medical signs and symptoms) at the most granular level. Diseases within MeSH are mapped to HPO to infer the phenotypic abnormalities associated with diseases. Building on the rich semantic phenotype annotation layer, we are able to make cross-species phenotype comparisons and publish a core subset of GWAS data as RDF nanopublications.&lt;h4&gt;Conclusions&lt;/h4&gt;We present a methodology for applying phenotype annotations to a comprehensive genome-wide association dataset and for ensuring compatibility with the Semantic Web. The annotations are used to assist with cross-species genotype and phenotype comparisons. However, further processing and deconstructions of terms may be required to facilitate automatic phenotype comparisons. The provision of GWAS nanopublications enables a new dimension for exploring GWAS data, by way of intrinsic links to related data resources within the Linked Data web. The value of such annotation and integration will grow as more biomedical resources adopt the standards of the Semantic Web."> Abstract: 331 words, </span></li><li><span class="pmcid"> PMC4015876, </span><span class="author_string"> Masseroli M, Mons B, Bongcam-Rudloff E, Ceri S, Kel A, Rechenmann F, Lisacek F, Romano P., </span><span class="title"> "Integrated Bio-Search: challenges and trends for the integration, search and comprehensive processing of biological information.", </span><span><b> 15 Suppl 1 </b></span><span> (2014): </span><span><i> BMC bioinformatics, </i></span><span class="page_info">PI S2, </span><a href="https://doi.org/10.1186/1471-2105-15-s1-s2"> DOI: 10.1186/1471-2105-15-s1-s2, </a><span class="abstract_text" title="Many efforts exist to design and implement approaches and tools for data capture, integration and analysis in the life sciences. Challenges are not only the heterogeneity, size and distribution of information sources, but also the danger of producing too many solutions for the same problem. Methodological, technological, infrastructural and social aspects appear to be essential for the development of a new generation of best practices and tools. In this paper, we analyse and discuss these aspects from different perspectives, by extending some of the ideas that arose during the NETTAB 2012 Workshop, making reference especially to the European context. First, relevance of using data and software models for the management and analysis of biological data is stressed. Second, some of the most relevant community achievements of the recent years, which should be taken as a starting point for future efforts in this research domain, are presented. Third, some of the main outstanding issues, challenges and trends are analysed. The challenges related to the tendency to fund and create large scale international research infrastructures and public-private partnerships in order to address the complex challenges of data intensive science are especially discussed. The needs and opportunities of Genomic Computing (the integration, search and display of genomic information at a very specific level, e.g. at the level of a single DNA region) are then considered. In the current data and network-driven era, social aspects can become crucial bottlenecks. How these may best be tackled to unleash the technical abilities for effective data integration and validation efforts is then discussed. Especially the apparent lack of incentives for already overwhelmed researchers appears to be a limitation for sharing information and knowledge with other scientists. We point out as well how the bioinformatics market is growing at an unprecedented speed due to the impact that new powerful in silico analysis promises to have on better diagnosis, prognosis, drug discovery and treatment, towards personalized medicine. An open business model for bioinformatics, which appears to be able to reduce undue duplication of efforts and support the increased reuse of valuable data sets, tools and platforms, is finally discussed."> Abstract: 350 words, </span></li><li><span class="pmcid"> PMC5753270, </span><span class="author_string"> Slenter DN, Kutmon M, Hanspers K, Riutta A, Windsor J, Nunes N, M&#233;lius J, Cirillo E, Coort SL, Digles D, Ehrhart F, Giesbertz P, Kalafati M, Martens M, Miller R, Nishida K, Rieswijk L, Waagmeester A, Eijssen LMT, Evelo CT, Pico AR, Willighagen EL., </span><span class="title"> "WikiPathways: a multifaceted pathway database bridging metabolomics to other omics research.", </span><span><b> 46(D1) </b></span><span> (2018): </span><span><i> Nucleic acids research, </i></span><span class="page_info">PI D661-D667, </span><a href="https://doi.org/10.1093/nar/gkx1064"> DOI: 10.1093/nar/gkx1064, </a><span class="abstract_text" title="WikiPathways (wikipathways.org) captures the collective knowledge represented in biological pathways. By providing a database in a curated, machine readable way, omics data analysis and visualization is enabled. WikiPathways and other pathway databases are used to analyze experimental data by research groups in many fields. Due to the open and collaborative nature of the WikiPathways platform, our content keeps growing and is getting more accurate, making WikiPathways a reliable and rich pathway database. Previously, however, the focus was primarily on genes and proteins, leaving many metabolites with only limited annotation. Recent curation efforts focused on improving the annotation of metabolism and metabolic pathways by associating unmapped metabolites with database identifiers and providing more detailed interaction knowledge. Here, we report the outcomes of the continued growth and curation efforts, such as a doubling of the number of annotated metabolite nodes in WikiPathways. Furthermore, we introduce an OpenAPI documentation of our web services and the FAIR (Findable, Accessible, Interoperable and Reusable) annotation of resources to increase the interoperability of the knowledge encoded in these pathways and experimental omics data. New search options, monthly downloads, more links to metabolite databases, and new portals make pathway knowledge more effortlessly accessible to individual researchers and research communities."> Abstract: 201 words, </span></li><li><span class="pmcid"> PMC4530550, </span><span class="author_string"> Clark T, Ciccarese PN, Goble CA., </span><span class="title"> "Micropublications: a semantic model for claims, evidence, arguments and annotations in biomedical communications.", </span><span><b> 5 </b></span><span> (2014): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 28, </span><a href="https://doi.org/10.1186/2041-1480-5-28"> DOI: 10.1186/2041-1480-5-28, </a><span class="abstract_text" title='&lt;h4&gt;Background&lt;/h4&gt;Scientific publications are documentary representations of defeasible arguments, supported by data and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications. The institutional "goal" of science is publishing results. The linear document publication format, dating from 1665, has survived transition to the Web. Intractable publication volumes; the difficulty of verifying evidence; and observed problems in evidence and citation chains suggest a need for a web-friendly and machine-tractable model of scientific publications. This model should support: digital summarization, evidence examination, challenge, verification and remix, and incremental adoption. Such a model must be capable of expressing a broad spectrum of representational complexity, ranging from minimal to maximal forms.&lt;h4&gt;Results&lt;/h4&gt;The micropublications semantic model of scientific argument and evidence provides these features. Micropublications support natural language statements; data; methods and materials specifications; discussion and commentary; challenge and disagreement; as well as allowing many kinds of statement formalization. The minimal form of a micropublication is a statement with its attribution. The maximal form is a statement with its complete supporting argument, consisting of all relevant evidence, interpretations, discussion and challenges brought forward in support of or opposition to it. Micropublications may be formalized and serialized in multiple ways, including in RDF. They may be added to publications as stand-off metadata. An OWL 2 vocabulary for micropublications is available at http://purl.org/mp. A discussion of this vocabulary along with RDF examples from the case studies, appears as OWL Vocabulary and RDF Examples in Additional file 1.&lt;h4&gt;Conclusion&lt;/h4&gt;Micropublications, because they model evidence and allow qualified, nuanced assertions, can play essential roles in the scientific communications ecosystem in places where simpler, formalized and purely statement-based models, such as the nanopublications model, will not be sufficient. At the same time they will add significant value to, and are intentionally compatible with, statement-based formalizations. We suggest that micropublications, generated by useful software tools supporting such activities as writing, editing, reviewing, and discussion, will be of great value in improving the quality and tractability of biomedical communications.'> Abstract: 328 words, </span></li><li><span class="pmcid"> PMC8573700, </span><span class="author_string"> Lamprecht AL, Palmblad M, Ison J, Schw&#228;mmle V, Al Manir MS, Altintas I, Baker CJO, Ben Hadj Amor A, Capella-Gutierrez S, Charonyktakis P, Crusoe MR, Gil Y, Goble C, Griffin TJ, Groth P, Ienasescu H, Jagtap P, Kala&#353; M, Kasalica V, Khanteymoori A, Kuhn T, Mei H, M&#233;nager H, M&#246;ller S, Richardson RA, Robert V, Soiland-Reyes S, Stevens R, Szaniszlo S, Verberne S, Verhoeven A, Wolstencroft K., </span><span class="title"> "Perspectives on automated composition of workflows in the life sciences.", </span><span><b> 10 </b></span><span> (2021): </span><span><i> F1000Research, </i></span><span class="page_info">PI 897, </span><a href="https://doi.org/10.12688/f1000research.54159.1"> DOI: 10.12688/f1000research.54159.1, </a><span class="abstract_text" title='Scientific data analyses often combine several computational tools in automated pipelines, or workflows. Thousands of such workflows have been used in the life sciences, though their composition has remained a cumbersome manual process due to a lack of standards for annotation, assembly, and implementation. Recent technological advances have returned the long-standing vision of automated workflow composition into focus. This article summarizes a recent Lorentz Center workshop dedicated to automated composition of workflows in the life sciences. We survey previous initiatives to automate the composition process, and discuss the current state of the art and future perspectives. We start by drawing the "big picture" of the scientific workflow development life cycle, before surveying and discussing current methods, technologies and practices for semantic domain modelling, automation in workflow development, and workflow assessment. Finally, we derive a roadmap of individual and community-based actions to work toward the vision of automated workflow development in the forthcoming years. A central outcome of the workshop is a general description of the workflow life cycle in six stages: 1) scientific question or hypothesis, 2) conceptual workflow, 3) abstract workflow, 4) concrete workflow, 5) production workflow, and 6) scientific results. The transitions between stages are facilitated by diverse tools and methods, usually incorporating domain knowledge in some form. Formal semantic domain modelling is hard and often a bottleneck for the application of semantic technologies. However, life science communities have made considerable progress here in recent years and are continuously improving, renewing interest in the application of semantic technologies for workflow exploration, composition and instantiation. Combined with systematic benchmarking with reference data and large-scale deployment of production-stage workflows, such technologies enable a more systematic process of workflow development than we know today. We believe that this can lead to more robust, reusable, and sustainable workflows in the future.'> Abstract: 299 words, </span></li><li><span class="pmcid"> PMC9603803, </span><span class="author_string"> Zhao L, Zhao Y, Wu Y, Ding X, Yu F, Peng K., </span><span class="title"> "Impacts of the COVID-19 Pandemic upon Chinese Positive Traits.", </span><span><b> 19(20) </b></span><span> (2022): </span><span><i> International journal of environmental research and public health, </i></span><span class="page_info">PI 13490, </span><a href="https://doi.org/10.3390/ijerph192013490"> DOI: 10.3390/ijerph192013490, </a><span class="abstract_text" title="Will Chinese people change in terms of their character strengths when disasters strike? As far as the most recent COVID-19 pandemic is concerned, we provide an explorative answer from the impacts of positive traits included in the Values in Action Classification of Strengths upon Chinese people. We conducted a large-scale online survey from 1 January 2019 to 13 February 2020, with 12,878 respondents nationwide, covering all the administrative regions in China and all age intervals. The changes in the 24 character strengths before and during the pandemic were compared. Results revealed a significant increase in teamwork triggered by the pandemic among Chinese people. Fine-grained differences in demographic variables were also examined. Results showed that the COVID-19 pandemic significantly boosted teamwork for both males and females. Concerning age differences, only younger adults (18-25-year-old) showed a significant increase in teamwork. Besides this, it was also discovered that females always performed a higher teamwork tendency than males, and the elderly higher than the younger, regardless of the pandemic."> Abstract: 165 words, </span></li><li><span class="pmcid"> PMC5333274, </span><span class="author_string"> Maldonado JA, Marcos M, Fern&#225;ndez-Breis JT, Parcero E, Bosc&#225; D, Legaz-Garc&#237;a MD, Mart&#237;nez-Salvador B, Robles M., </span><span class="title"> "A platform for exploration into chaining of web services for clinical data transformation and reasoning.", </span><span><b> 2016 </b></span><span> (2016): </span><span><i> AMIA ... Annual Symposium proceedings. AMIA Symposium, </i></span><span class="page_info">PI 854-863, </span><span class="abstract_text" title="The heterogeneity of clinical data is a key problem in the sharing and reuse of Electronic Health Record (EHR) data. We approach this problem through the combined use of EHR standards and semantic web technologies, concretely by means of clinical data transformation applications that convert EHR data in proprietary format, first into clinical information models based on archetypes, and then into RDF/OWL extracts which can be used for automated reasoning. In this paper we describe a proof-of-concept platform to facilitate the (re)configuration of such clinical data transformation applications. The platform is built upon a number of web services dealing with transformations at different levels (such as normalization or abstraction), and relies on a collection of reusable mappings designed to solve specific transformation steps in a particular clinical domain. The platform has been used in the development of two different data transformation applications in the area of colorectal cancer."> Abstract: 148 words, </span></li><li><span class="pmcid"> PMC7756623, </span><span class="author_string"> Neal ML, Gennari JH, Waltemath D, Nickerson DP, K&#246;nig M., </span><span class="title"> "Open modeling and exchange (OMEX) metadata specification version 1.0.", </span><span><b> 17(2-3) </b></span><span> (2020): </span><span><i> Journal of integrative bioinformatics, </i></span><a href="https://doi.org/10.1515/jib-2020-0020"> DOI: 10.1515/jib-2020-0020, </a><span class="abstract_text" title="A standardized approach to annotating computational biomedical models and their associated files can facilitate model reuse and reproducibility among research groups, enhance search and retrieval of models and data, and enable semantic comparisons between models. Motivated by these potential benefits and guided by consensus across the COmputational Modeling in BIology NEtwork (COMBINE) community, we have developed a specification for encoding annotations in Open Modeling and EXchange (OMEX)-formatted archives. Distributing modeling projects within these archives is a best practice established by COMBINE, and the OMEX metadata specification presented here provides a harmonized, community-driven approach for annotating a variety of standardized model and data representation formats within an archive. The specification primarily includes technical guidelines for encoding archive metadata, so that software tools can more easily utilize and exchange it, thereby spurring broad advancements in model reuse, discovery, and semantic analyses."> Abstract: 139 words, </span></li><li><span class="pmcid"> PMC11087458, </span><span class="author_string"> Corcho O, Ekaputra FJ, Heibi I, Jonquet C, Micsik A, Peroni S, Storti E., </span><span class="title"> "A maturity model for catalogues of semantic artefacts.", </span><span><b> 11(1) </b></span><span> (2024): </span><span><i> Scientific data, </i></span><span class="page_info">PI 479, </span><a href="https://doi.org/10.1038/s41597-024-03185-4"> DOI: 10.1038/s41597-024-03185-4, </a><span class="abstract_text" title="This work presents a maturity model for assessing catalogues of semantic artefacts, one of the keystones that permit semantic interoperability of systems. We defined the dimensions and related features to include in the maturity model by analysing the current literature and existing catalogues of semantic artefacts provided by experts. In addition, we assessed 26 different catalogues to demonstrate the effectiveness of the maturity model, which includes 12 different dimensions (Metadata, Openness, Quality, Availability, Statistics, PID, Governance, Community, Sustainability, Technology, Transparency, and Assessment) and 43 related features (or sub-criteria) associated with these dimensions. Such a maturity model is one of the first attempts to provide recommendations for governance and processes for preserving and maintaining semantic artefacts and helps assess/address interoperability challenges."> Abstract: 120 words, </span></li><li><span class="pmcid"> PMC5019567, </span><span class="author_string"> McCusker JP, Lebo T, Krauthammer M, McGuinness DL., </span><span class="title"> "Next Generation Cancer Data Discovery, Access, and Integration Using Prizms and Nanopublications.", </span><span><b> 7970 </b></span><span> (2013): </span><span><i> Data integration in the life sciences : ... International Workshop, DILS ... : proceedings. DILS (Conference), </i></span><span class="page_info">PI 105-112, </span><a href="https://doi.org/10.1007/978-3-642-39437-9_9"> DOI: 10.1007/978-3-642-39437-9_9, </a><span class="abstract_text" title="To encourage data sharing in the life sciences, supporting tools need to minimize effort and maximize incentives. We have created infrastructure that makes it easy to create portals that supports dataset sharing and simplified publishing of the datasets as high quality linked data. We report here on our infrastructure and its use in the creation of a melanoma dataset portal. This portal is based on the Comprehensive Knowledge Archive Network (CKAN) and Prizms, an infrastructure to acquire, integrate, and publish data using Linked Data principles. In addition, we introduce an extension to CKAN that makes it easy for others to cite datasets from within both publications and subsequently-derived datasets using the emerging nanopublication and World Wide Web Consortium provenance standards."> Abstract: 120 words, </span></li><li><span class="pmcid"> PMC8843814, </span><span class="author_string"> Altman M, Cohen PN., </span><span class="title"> "The Scholarly Knowledge Ecosystem: Challenges and Opportunities for the Field of Information.", </span><span><b> 6 </b></span><span> (2021): </span><span><i> Frontiers in research metrics and analytics, </i></span><span class="page_info">PI 751553, </span><a href="https://doi.org/10.3389/frma.2021.751553"> DOI: 10.3389/frma.2021.751553, </a><span class="abstract_text" title="The scholarly knowledge ecosystem presents an outstanding exemplar of the challenges of understanding, improving, and governing information ecosystems at scale. This article draws upon significant reports on aspects of the ecosystem to characterize the most important research challenges and promising potential approaches. The focus of this review article is the fundamental scientific research challenges related to developing a better understanding of the scholarly knowledge ecosystem. Across a range of disciplines, we identify reports that are conceived broadly, published recently, and written collectively. We extract the critical research questions, summarize these using quantitative text analysis, and use this quantitative analysis to inform a qualitative synthesis. Three broad themes emerge from this analysis: the need for multi-sectoral cooperation and coordination, for mixed methods analysis at multiple levels, and interdisciplinary collaboration. Further, we draw attention to an emerging consensus that scientific research in this area should by a set of core human values."> Abstract: 150 words, </span></li><li><span class="pmcid"> PMC5774086, </span><span class="author_string"> Senderov V, Simov K, Franz N, Stoev P, Catapano T, Agosti D, Sautter G, Morris RA, Penev L., </span><span class="title"> "OpenBiodiv-O: ontology of the OpenBiodiv knowledge management system.", </span><span><b> 9(1) </b></span><span> (2018): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 5, </span><a href="https://doi.org/10.1186/s13326-017-0174-5"> DOI: 10.1186/s13326-017-0174-5, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;The biodiversity domain, and in particular biological taxonomy, is moving in the direction of semantization of its research outputs. The present work introduces OpenBiodiv-O, the ontology that serves as the basis of the OpenBiodiv Knowledge Management System. Our intent is to provide an ontology that fills the gaps between ontologies for biodiversity resources, such as DarwinCore-based ontologies, and semantic publishing ontologies, such as the SPAR Ontologies. We bridge this gap by providing an ontology focusing on biological taxonomy.&lt;h4&gt;Results&lt;/h4&gt;OpenBiodiv-O introduces classes, properties, and axioms in the domains of scholarly biodiversity publishing and biological taxonomy and aligns them with several important domain ontologies (FaBiO, DoCO, DwC, Darwin-SW, NOMEN, ENVO). By doing so, it bridges the ontological gap across scholarly biodiversity publishing and biological taxonomy and allows for the creation of a Linked Open Dataset (LOD) of biodiversity information (a biodiversity knowledge graph) and enables the creation of the OpenBiodiv Knowledge Management System. A key feature of the ontology is that it is an ontology of the scientific process of biological taxonomy and not of any particular state of knowledge. This feature allows it to express a multiplicity of scientific opinions. The resulting OpenBiodiv knowledge system may gain a high level of trust in the scientific community as it does not force a scientific opinion on its users (e.g. practicing taxonomists, library researchers, etc.), but rather provides the tools for experts to encode different views as science progresses.&lt;h4&gt;Conclusions&lt;/h4&gt;OpenBiodiv-O provides a conceptual model of the structure of a biodiversity publication and the development of related taxonomic concepts. It also serves as the basis for the OpenBiodiv Knowledge Management System."> Abstract: 265 words, </span></li><li><span class="pmcid"> PMC6110999, </span><span class="author_string"> Allard PM, Bisson J, Azzollini A, Pauli GF, Cordell GA, Wolfender JL., </span><span class="title"> "Pharmacognosy in the digital era: shifting to contextualized metabolomics.", </span><span><b> 54 </b></span><span> (2018): </span><span><i> Current opinion in biotechnology, </i></span><span class="page_info">PI 57-64, </span><a href="https://doi.org/10.1016/j.copbio.2018.02.010"> DOI: 10.1016/j.copbio.2018.02.010, </a><span class="abstract_text" title="Humans have co-evolved alongside numerous other organisms, some having a profound effect on health and nutrition. As the earliest pharmaceutical subject, pharmacognosy has evolved into a meta-discipline devoted to natural biomedical agents and their functional properties. While the acquisition of expanding data volumes is ongoing, contextualization is lagging. Thus, we assert that the establishment of an integrated and open databases ecosystem will nurture the discipline. After proposing an epistemological framework of knowledge acquisition in pharmacognosy, this study focuses on recent computational and analytical approaches. It then elaborates on the flux of research data, where good practices could foster the implementation of more integrated systems, which will in turn help shaping the future of pharmacognosy and determine its constitutional societal relevance."> Abstract: 120 words, </span></li><li><span class="pmcid"> PMC5836261, </span><span class="author_string"> Raciti D, Yook K, Harris TW, Schedl T, Sternberg PW., </span><span class="title"> "Micropublication: incentivizing community curation and placing unpublished data into the public domain.", </span><span><b> 2018 </b></span><span> (2018): </span><span><i> Database : the journal of biological databases and curation, </i></span><a href="https://doi.org/10.1093/database/bay013"> DOI: 10.1093/database/bay013, </a><span class="abstract_text" title="Large volumes of data generated by research laboratories coupled with the required effort and cost of curation present a significant barrier to inclusion of these data in authoritative community databases. Further, many publicly funded experimental observations remain invisible to curation simply because they are never published: results often do not fit within the scope of a standard publication; trainee-generated data are forgotten when the experimenter (e.g. student, post-doc) leaves the lab; results are omitted from science narratives due to publication bias where certain results are considered irrelevant for the publication. While authors are in the best position to curate their own data, they face a steep learning curve to ensure that appropriate referential tags, metadata, and ontologies are applied correctly to their observations, a task sometimes considered beyond the scope of their research and other numerous responsibilities. Getting researchers to adopt a new system of data reporting and curation requires a fundamental change in behavior among all members of the research community. To solve these challenges, we have created a novel scholarly communication platform that captures data from researchers and directly delivers them to information resources via Micropublication. This platform incentivizes authors to publish their unpublished observations along with associated metadata by providing a deliberately fast and lightweight but still peer-reviewed process that results in a citable publication. Our long-term goal is to develop a data ecosystem that improves reproducibility and accountability of publicly funded research and in turn accelerates both basic and translational discovery.&lt;h4&gt;Database url&lt;/h4&gt;www.micropublication.org."> Abstract: 246 words, </span></li><li><span class="pmcid"> PMC4177195, </span><span class="author_string"> Ciccarese P, Soiland-Reyes S, Belhajjame K, Gray AJ, Goble C, Clark T., </span><span class="title"> "PAV ontology: provenance, authoring and versioning.", </span><span><b> 4(1) </b></span><span> (2013): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 37, </span><a href="https://doi.org/10.1186/2041-1480-4-37"> DOI: 10.1186/2041-1480-4-37, </a><span class="abstract_text" title='&lt;h4&gt;Background&lt;/h4&gt;Provenance is a critical ingredient for establishing trust of published scientific content. This is true whether we are considering a data set, a computational workflow, a peer-reviewed publication or a simple scientific claim with supportive evidence. Existing vocabularies such as Dublin Core Terms (DC Terms) and the W3C Provenance Ontology (PROV-O) are domain-independent and general-purpose and they allow and encourage for extensions to cover more specific needs. In particular, to track authoring and versioning information of web resources, PROV-O provides a basic methodology but not any specific classes and properties for identifying or distinguishing between the various roles assumed by agents manipulating digital artifacts, such as author, contributor and curator.&lt;h4&gt;Results&lt;/h4&gt;We present the Provenance, Authoring and Versioning ontology (PAV, namespace http://purl.org/pav/): a lightweight ontology for capturing "just enough" descriptions essential for tracking the provenance, authoring and versioning of web resources. We argue that such descriptions are essential for digital scientific content. PAV distinguishes between contributors, authors and curators of content and creators of representations in addition to the provenance of originating resources that have been accessed, transformed and consumed. We explore five projects (and communities) that have adopted PAV illustrating their usage through concrete examples. Moreover, we present mappings that show how PAV extends the W3C PROV-O ontology to support broader interoperability.&lt;h4&gt;Method&lt;/h4&gt;The initial design of the PAV ontology was driven by requirements from the AlzSWAN project with further requirements incorporated later from other projects detailed in this paper. The authors strived to keep PAV lightweight and compact by including only those terms that have demonstrated to be pragmatically useful in existing applications, and by recommending terms from existing ontologies when plausible.&lt;h4&gt;Discussion&lt;/h4&gt;We analyze and compare PAV with related approaches, namely Provenance Vocabulary (PRV), DC Terms and BIBFRAME. We identify similarities and analyze differences between those vocabularies and PAV, outlining strengths and weaknesses of our proposed model. We specify SKOS mappings that align PAV with DC Terms. We conclude the paper with general remarks on the applicability of PAV.'> Abstract: 326 words, </span></li><li><span class="pmcid"> PMC10783158, </span><span class="author_string"> Samuel S, Mietchen D., </span><span class="title"> "Computational reproducibility of Jupyter notebooks from biomedical publications.", </span><span><b> 13 </b></span><span> (2024): </span><span><i> GigaScience, </i></span><span class="page_info">PI giad113, </span><a href="https://doi.org/10.1093/gigascience/giad113"> DOI: 10.1093/gigascience/giad113, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Jupyter notebooks facilitate the bundling of executable code with its documentation and output in one interactive environment, and they represent a popular mechanism to document and share computational workflows, including for research publications. The reproducibility of computational aspects of research is a key component of scientific reproducibility but has not yet been assessed at scale for Jupyter notebooks associated with biomedical publications.&lt;h4&gt;Approach&lt;/h4&gt;We address computational reproducibility at 2 levels: (i) using fully automated workflows, we analyzed the computational reproducibility of Jupyter notebooks associated with publications indexed in the biomedical literature repository PubMed Central. We identified such notebooks by mining the article's full text, trying to locate them on GitHub, and attempting to rerun them in an environment as close to the original as possible. We documented reproduction success and exceptions and explored relationships between notebook reproducibility and variables related to the notebooks or publications. (ii) This study represents a reproducibility attempt in and of itself, using essentially the same methodology twice on PubMed Central over the course of 2 years, during which the corpus of Jupyter notebooks from articles indexed in PubMed Central has grown in a highly dynamic fashion.&lt;h4&gt;Results&lt;/h4&gt;Out of 27,271 Jupyter notebooks from 2,660 GitHub repositories associated with 3,467 publications, 22,578 notebooks were written in Python, including 15,817 that had their dependencies declared in standard requirement files and that we attempted to rerun automatically. For 10,388 of these, all declared dependencies could be installed successfully, and we reran them to assess reproducibility. Of these, 1,203 notebooks ran through without any errors, including 879 that produced results identical to those reported in the original notebook and 324 for which our results differed from the originally reported ones. Running the other notebooks resulted in exceptions.&lt;h4&gt;Conclusions&lt;/h4&gt;We zoom in on common problems and practices, highlight trends, and discuss potential improvements to Jupyter-related workflows associated with biomedical publications."> Abstract: 304 words, </span></li><li><span class="pmcid"> PMC5210640, </span><span class="author_string"> Pi&#241;ero J, Bravo &#192;, Queralt-Rosinach N, Guti&#233;rrez-Sacrist&#225;n A, Deu-Pons J, Centeno E, Garc&#237;a-Garc&#237;a J, Sanz F, Furlong LI., </span><span class="title"> "DisGeNET: a comprehensive platform integrating information on human disease-associated genes and variants.", </span><span><b> 45(D1) </b></span><span> (2017): </span><span><i> Nucleic acids research, </i></span><span class="page_info">PI D833-D839, </span><a href="https://doi.org/10.1093/nar/gkw943"> DOI: 10.1093/nar/gkw943, </a><span class="abstract_text" title="The information about the genetic basis of human diseases lies at the heart of precision medicine and drug discovery. However, to realize its full potential to support these goals, several problems, such as fragmentation, heterogeneity, availability and different conceptualization of the data must be overcome. To provide the community with a resource free of these hurdles, we have developed DisGeNET (http://www.disgenet.org), one of the largest available collections of genes and variants involved in human diseases. DisGeNET integrates data from expert curated repositories, GWAS catalogues, animal models and the scientific literature. DisGeNET data are homogeneously annotated with controlled vocabularies and community-driven ontologies. Additionally, several original metrics are provided to assist the prioritization of genotype-phenotype relationships. The information is accessible through a web interface, a Cytoscape App, an RDF SPARQL endpoint, scripts in several programming languages and an R package. DisGeNET is a versatile platform that can be used for different research purposes including the investigation of the molecular underpinnings of specific human diseases and their comorbidities, the analysis of the properties of disease genes, the generation of hypothesis on drug therapeutic action and drug adverse effects, the validation of computationally predicted disease genes and the evaluation of text-mining methods performance."> Abstract: 199 words, </span></li><li><span class="pmcid"> PMC9836581, </span><span class="author_string"> Groom Q, Br&#228;uchler C, Cubey RWN, Dillen M, Huybrechts P, Kearney N, Klazenga N, Leachman S, Paul DL, Rogers H, Santos J, Shorthouse DP, Vaughan A, von Mering S, Haston EM., </span><span class="title"> "The disambiguation of people names in biological collections.", </span><span><b> 10 </b></span><span> (2022): </span><span><i> Biodiversity data journal, </i></span><span class="page_info">PI e86089, </span><a href="https://doi.org/10.3897/bdj.10.e86089"> DOI: 10.3897/bdj.10.e86089, </a><span class="abstract_text" title="Scientific collections have been built by people. For hundreds of years, people have collected, studied, identified, preserved, documented and curated collection specimens. Understanding who those people are is of interest to historians, but much more can be made of these data by other stakeholders once they have been linked to the people's identities and their biographies. Knowing who people are helps us attribute work correctly, validate data and understand the scientific contribution of people and institutions. We can evaluate the work they have done, the interests they have, the places they have worked and what they have created from the specimens they have collected. The problem is that all we know about most of the people associated with collections are their names written on specimens. Disambiguating these people is the challenge that this paper addresses. Disambiguation of people often proves difficult in isolation and can result in staff or researchers independently trying to determine the identity of specific individuals over and over again. By sharing biographical data and building an open, collectively maintained dataset with shared knowledge, expertise and resources, it is possible to collectively deduce the identities of individuals, aggregate biographical information for each person, reduce duplication of effort and share the information locally and globally. The authors of this paper aspire to disambiguate all person names efficiently and fully in all their variations across the entirety of the biological sciences, starting with collections. Towards that vision, this paper has three key aims: to improve the linking, validation, enhancement and valorisation of person-related information within and between collections, databases and publications; to suggest good practice for identifying people involved in biological collections; and to promote coordination amongst all stakeholders, including individuals, natural history collections, institutions, learned societies, government agencies and data aggregators."> Abstract: 293 words, </span></li><li><span class="pmcid"> PMC4255736, </span><span class="author_string"> Romano P, Cannata N., </span><span class="title"> "NETTAB 2013: Semantic, social, and mobile applications for bioinformatics and biomedical laboratories.", </span><span><b> 15 Suppl 14 </b></span><span> (2014): </span><span><i> BMC bioinformatics, </i></span><span class="page_info">PI S1, </span><a href="https://doi.org/10.1186/1471-2105-15-s14-s1"> DOI: 10.1186/1471-2105-15-s14-s1, </a><span class="abstract_text" title="The thirteenth NETTAB workshop, NETTAB 2013, was devoted to semantic, social, and mobile applications for bioinformatics and biomedical laboratories. Topics included issues, methods, algorithms, and technologies for the design and development of tools and platforms able to provide semantic, social, and mobile applications supporting bioinformatics and the activities carried out in a biomedical laboratory. About 30 scientific contributions were presentedat NETTAB 2013, including keynote and tutorial talks, oral communications, and posters. Best contributions presented at the workshop were later submitted to a special Call for this Supplement. Here, we provide an overview of the workshop and introduce manuscripts that have been accepted for publication in this Supplement."> Abstract: 107 words, </span></li><li><span class="pmcid"> PMC6291799, </span><span class="author_string"> Kilicoglu H., </span><span class="title"> "Biomedical text mining for research rigor and integrity: tasks, challenges, directions.", </span><span><b> 19(6) </b></span><span> (2018): </span><span><i> Briefings in bioinformatics, </i></span><span class="page_info">PI 1400-1414, </span><a href="https://doi.org/10.1093/bib/bbx057"> DOI: 10.1093/bib/bbx057, </a><span class="abstract_text" title="An estimated quarter of a trillion US dollars is invested in the biomedical research enterprise annually. There is growing alarm that a significant portion of this investment is wasted because of problems in reproducibility of research findings and in the rigor and integrity of research conduct and reporting. Recent years have seen a flurry of activities focusing on standardization and guideline development to enhance the reproducibility and rigor of biomedical research. Research activity is primarily communicated via textual artifacts, ranging from grant applications to journal publications. These artifacts can be both the source and the manifestation of practices leading to research waste. For example, an article may describe a poorly designed experiment, or the authors may reach conclusions not supported by the evidence presented. In this article, we pose the question of whether biomedical text mining techniques can assist the stakeholders in the biomedical research enterprise in doing their part toward enhancing research integrity and rigor. In particular, we identify four key areas in which text mining techniques can make a significant contribution: plagiarism/fraud detection, ensuring adherence to reporting guidelines, managing information overload and accurate citation/enhanced bibliometrics. We review the existing methods and tools for specific tasks, if they exist, or discuss relevant research that can provide guidance for future work. With the exponential increase in biomedical research output and the ability of text mining approaches to perform automatic tasks at large scale, we propose that such approaches can support tools that promote responsible research practices, providing significant benefits for the biomedical research enterprise."> Abstract: 254 words, </span></li><li><span class="pmcid"> PMC7182025, </span><span class="author_string"> Prieto M, Deus H, de Waard A, Schultes E, Garc&#237;a-Jim&#233;nez B, Wilkinson MD., </span><span class="title"> "Data-driven classification of the certainty of scholarly assertions.", </span><span><b> 8 </b></span><span> (2020): </span><span><i> PeerJ, </i></span><span class="page_info">PI e8871, </span><a href="https://doi.org/10.7717/peerj.8871"> DOI: 10.7717/peerj.8871, </a><span class="abstract_text" title="The grammatical structures scholars use to express their assertions are intended to convey various degrees of certainty or speculation. Prior studies have suggested a variety of categorization systems for scholarly certainty; however, these have not been objectively tested for their validity, particularly with respect to representing the interpretation by the reader, rather than the intention of the author. In this study, we use a series of questionnaires to determine how researchers classify various scholarly assertions, using three distinct certainty classification systems. We find that there are three distinct categories of certainty along a spectrum from high to low. We show that these categories can be detected in an automated manner, using a machine learning model, with a cross-validation accuracy of 89.2% relative to an author-annotated corpus, and 82.2% accuracy against a publicly-annotated corpus. This finding provides an opportunity for contextual metadata related to certainty to be captured as a part of text-mining pipelines, which currently miss these subtle linguistic cues. We provide an exemplar machine-accessible representation-a Nanopublication-where certainty category is embedded as metadata in a formal, ontology-based manner within text-mined scholarly assertions."> Abstract: 182 words, </span></li><li><span class="pmcid"> PMC7594074, </span><span class="author_string"> Ammar A, Bonaretti S, Winckers L, Quik J, Bakker M, Maier D, Lynch I, van Rijn J, Willighagen E., </span><span class="title"> "A Semi-Automated Workflow for FAIR Maturity Indicators in the Life Sciences.", </span><span><b> 10(10) </b></span><span> (2020): </span><span><i> Nanomaterials (Basel, Switzerland), </i></span><span class="page_info">PI E2068, </span><a href="https://doi.org/10.3390/nano10102068"> DOI: 10.3390/nano10102068, </a><span class="abstract_text" title="Data sharing and reuse are crucial to enhance scientific progress and maximize return of investments in science. Although attitudes are increasingly favorable, data reuse remains difficult due to lack of infrastructures, standards, and policies. The FAIR (findable, accessible, interoperable, reusable) principles aim to provide recommendations to increase data reuse. Because of the broad interpretation of the FAIR principles, maturity indicators are necessary to determine the FAIRness of a dataset. In this work, we propose a reproducible computational workflow to assess data FAIRness in the life sciences. Our implementation follows principles and guidelines recommended by the maturity indicator authoring group and integrates concepts from the literature. In addition, we propose a FAIR balloon plot to summarize and compare dataset FAIRness. We evaluated the feasibility of our method on three real use cases where researchers looked for six datasets to answer their scientific questions. We retrieved information from repositories (ArrayExpress, Gene Expression Omnibus, eNanoMapper, caNanoLab, NanoCommons and ChEMBL), a registry of repositories, and a searchable resource (Google Dataset Search) via application program interfaces (API) wherever possible. With our analysis, we found that the six datasets met the majority of the criteria defined by the maturity indicators, and we showed areas where improvements can easily be reached. We suggest that use of standard schema for metadata and the presence of specific attributes in registries of repositories could increase FAIRness of datasets."> Abstract: 228 words, </span></li><li><span class="pmcid"> PMC5210666, </span><span class="author_string"> Lizio M, Harshbarger J, Abugessaisa I, Noguchi S, Kondo A, Severin J, Mungall C, Arenillas D, Mathelier A, Medvedeva YA, Lennartsson A, Drabl&#248;s F, Ramilowski JA, Rackham O, Gough J, Andersson R, Sandelin A, Ienasescu H, Ono H, Bono H, Hayashizaki Y, Carninci P, Forrest AR, Kasukawa T, Kawaji H., </span><span class="title"> "Update of the FANTOM web resource: high resolution transcriptome of diverse cell types in mammals.", </span><span><b> 45(D1) </b></span><span> (2017): </span><span><i> Nucleic acids research, </i></span><span class="page_info">PI D737-D743, </span><a href="https://doi.org/10.1093/nar/gkw995"> DOI: 10.1093/nar/gkw995, </a><span class="abstract_text" title="Upon the first publication of the fifth iteration of the Functional Annotation of Mammalian Genomes collaborative project, FANTOM5, we gathered a series of primary data and database systems into the FANTOM web resource (http://fantom.gsc.riken.jp) to facilitate researchers to explore transcriptional regulation and cellular states. In the course of the collaboration, primary data and analysis results have been expanded, and functionalities of the database systems enhanced. We believe that our data and web systems are invaluable resources, and we think the scientific community will benefit for this recent update to deepen their understanding of mammalian cellular organization. We introduce the contents of FANTOM5 here, report recent updates in the web resource and provide future perspectives."> Abstract: 114 words, </span></li><li><span class="pmcid"> PMC5171581, </span><span class="author_string"> Barros M, Couto FM., </span><span class="title"> "Knowledge Representation and Management: a Linked Data Perspective.", </span><span> (2016): </span><span><i> Yearbook of medical informatics, </i></span><span class="page_info">PI 178-183, </span><a href="https://doi.org/10.15265/iy-2016-022"> DOI: 10.15265/iy-2016-022, </a><span class="abstract_text" title="&lt;h4&gt;Introduction&lt;/h4&gt;Biomedical research is increasingly becoming a data-intensive science in several areas, where prodigious amounts of data is being generated that has to be stored, integrated, shared and analyzed. In an effort to improve the accessibility of data and knowledge, the Linked Data initiative proposed a well-defined set of recommendations for exposing, sharing and integrating data, information and knowledge, using semantic web technologies.&lt;h4&gt;Objective&lt;/h4&gt;The main goal of this paper is to identify the current status and future trends of knowledge representation and management in Life and Health Sciences, mostly with regard to linked data technologies.&lt;h4&gt;Methods&lt;/h4&gt;We selected three prominent linked data studies, namely Bio2RDF, Open PHACTS and EBI RDF platform, and selected 14 studies published after 2014 (inclusive) that cited any of the three studies. We manually analyzed these 14 papers in relation to how they use linked data techniques.&lt;h4&gt;Results&lt;/h4&gt;The analyses show a tendency to use linked data techniques in Life and Health Sciences, and even if some studies do not follow all of the recommendations, many of them already represent and manage their knowledge using RDF and biomedical ontologies.&lt;h4&gt;Conclusion&lt;/h4&gt;These insights from RDF and biomedical ontologies are having a strong impact on how knowledge is generated from biomedical data, by making data elements increasingly connected and by providing a better description of their semantics. As health institutes become more data centric, we believe that the adoption of linked data techniques will continue to grow and be an effective solution to knowledge representation and management."> Abstract: 241 words, </span></li><li><span class="pmcid"> PMC6033003, </span><span class="author_string"> Townend GS, Ehrhart F, van Kranen HJ, Wilkinson M, Jacobsen A, Roos M, Willighagen EL, van Enckevort D, Evelo CT, Curfs LMG., </span><span class="title"> "MECP2 variation in Rett syndrome-An overview of current coverage of genetic and phenotype data within existing databases.", </span><span><b> 39(7) </b></span><span> (2018): </span><span><i> Human mutation, </i></span><span class="page_info">PI 914-924, </span><a href="https://doi.org/10.1002/humu.23542"> DOI: 10.1002/humu.23542, </a><span class="abstract_text" title="Rett syndrome (RTT) is a monogenic rare disorder that causes severe neurological problems. In most cases, it results from a loss-of-function mutation in the gene encoding methyl-CPG-binding protein 2 (MECP2). Currently, about 900 unique MECP2 variations (benign and pathogenic) have been identified and it is suspected that the different mutations contribute to different levels of disease severity. For researchers and clinicians, it is important that genotype-phenotype information is available to identify disease-causing mutations for diagnosis, to aid in clinical management of the disorder, and to provide counseling for parents. In this study, 13 genotype-phenotype databases were surveyed for their general functionality and availability of RTT-specific MECP2 variation data. For each database, we investigated findability and interoperability alongside practical user functionality, and type and amount of genetic and phenotype data. The main conclusions are that, as well as being challenging to find these databases and specific MECP2 variants held within, interoperability is as yet poorly developed and requires effort to search across databases. Nevertheless, we found several thousand online database entries for MECP2 variations and their associated phenotypes, diagnosis, or predicted variant effects, which is a good starting point for researchers and clinicians who want to provide, annotate, and use the data."> Abstract: 201 words, </span></li><li><span class="pmcid"> PMC5763506, </span><span class="author_string"> Sarntivijai S, Diehl AD, He Y., </span><span class="title"> "Cells in experimental life sciences - challenges and solution to the rapid evolution of knowledge.", </span><span><b> 18(Suppl 17) </b></span><span> (2017): </span><span><i> BMC bioinformatics, </i></span><span class="page_info">PI 560, </span><a href="https://doi.org/10.1186/s12859-017-1976-2"> DOI: 10.1186/s12859-017-1976-2, </a><span class="abstract_text" title="Cell cultures used in biomedical experiments come in the form of both sample biopsy primary cells, and maintainable immortalised cell lineages. The rise of bioinformatics and high-throughput technologies has led us to the requirement of ontology representation of cell types and cell lines. The Cell Ontology (CL) and Cell Line Ontology (CLO) have long been established as reference ontologies in the OBO framework. We have compiled a series of the challenges and the proposals of solutions in this CELLS (Cells in ExperimentaL Life Sciences) thematic series that cover the grounds of standing issues and the directions, which were discussed in the First International Workshop on CELLS at the the International Conference on Biomedical Ontology (ICBO). This workshop focused on the extension of the current CL and CLO to cover a wider set of biological questions and challenges needing semantic infrastructure for information modeling. We discussed data-driven use cases that leverage linkage of CL, CLO and other bio-ontologies. This is an established approach in data-driven ontologies such as the Experimental Factor Ontology (EFO), and the Ontology for Biomedical Investigation (OBI). The First International Workshop on CELLS at the International Conference on Biomedical Ontology has brought together experimental biologists and biomedical ontologists to discuss solutions to organizing and representing the rapidly evolving knowledge gained from experimental cells. The workshop has successfully identified the areas of challenge, and the gap in connecting the two domains of knowledge. The outcome of this workshop yielded practical implementation plans to filled in this gap.This CELLS workshop also provided a venue for panel discussions of innovative solutions as well as challenges in the development and applications of biomedical ontologies to represent and analyze experimental cell data."> Abstract: 279 words, </span></li><li><span class="pmcid"> PMC2682198, </span><span class="author_string"> Markel S., </span><span class="title"> "BioLINK special interest group session on the future of scientific publishing.", </span><span><b> 5(5) </b></span><span> (2009): </span><span><i> PLoS computational biology, </i></span><span class="page_info">PI e1000398, </span><a href="https://doi.org/10.1371/journal.pcbi.1000398"> DOI: 10.1371/journal.pcbi.1000398, </a></li><li><span class="pmcid"> PMC3102892, </span><span class="author_string"> Callahan A, Dumontier M, Shah NH., </span><span class="title"> "HyQue: evaluating hypotheses using Semantic Web technologies.", </span><span><b> 2 Suppl 2 </b></span><span> (2011): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI S3, </span><a href="https://doi.org/10.1186/2041-1480-2-s2-s3"> DOI: 10.1186/2041-1480-2-s2-s3, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Key to the success of e-Science is the ability to computationally evaluate expert-composed hypotheses for validity against experimental data. Researchers face the challenge of collecting, evaluating and integrating large amounts of diverse information to compose and evaluate a hypothesis. Confronted with rapidly accumulating data, researchers currently do not have the software tools to undertake the required information integration tasks.&lt;h4&gt;Results&lt;/h4&gt;We present HyQue, a Semantic Web tool for querying scientific knowledge bases with the purpose of evaluating user submitted hypotheses. HyQue features a knowledge model to accommodate diverse hypotheses structured as events and represented using Semantic Web languages (RDF/OWL). Hypothesis validity is evaluated against experimental and literature-sourced evidence through a combination of SPARQL queries and evaluation rules. Inference over OWL ontologies (for type specifications, subclass assertions and parthood relations) and retrieval of facts stored as Bio2RDF linked data provide support for a given hypothesis. We evaluate hypotheses of varying levels of detail about the genetic network controlling galactose metabolism in Saccharomyces cerevisiae to demonstrate the feasibility of deploying such semantic computing tools over a growing body of structured knowledge in Bio2RDF.&lt;h4&gt;Conclusions&lt;/h4&gt;HyQue is a query-based hypothesis evaluation system that can currently evaluate hypotheses about the galactose metabolism in S. cerevisiae. Hypotheses as well as the supporting or refuting data are represented in RDF and directly linked to one another allowing scientists to browse from data to hypothesis and vice versa. HyQue hypotheses and data are available at http://semanticscience.org/projects/hyque."> Abstract: 235 words, </span></li><li><span class="pmcid"> PMC7198318, </span><span class="author_string"> Buneman P, Christie G, Davies JA, Dimitrellou R, Harding SD, Pawson AJ, Sharman JL, Wu Y., </span><span class="title"> "Why data citation isn't working, and what to do about it.", </span><span><b> 2020 </b></span><span> (2020): </span><span><i> Database : the journal of biological databases and curation, </i></span><span class="page_info">PI baaa022, </span><a href="https://doi.org/10.1093/databa/baaa022"> DOI: 10.1093/databa/baaa022, </a><span class="abstract_text" title="We describe a system that automatically generates from a curated database a collection of short conventional publications-citation summaries-that describe the contents of various components of the database. The purpose of these summaries is to ensure that the contributors to the database receive appropriate credit through the currently used measures such as h-indexes. Moreover, these summaries also serve to give credit to publications and people that are cited by the database. In doing this, we need to deal with granularity-how many summaries should be generated to represent effectively the contributions to a database? We also need to deal with evolution-for how long can a given summary serve as an appropriate reference when the database is evolving? We describe a journal specifically tailored to contain these citation summaries. We also briefly discuss the limitations that the current mechanisms for recording citations place on both the process and value of data citation."> Abstract: 149 words, </span></li><li><span class="pmcid"> PMC11179050, </span><span class="author_string"> Waterhouse RM, Adam-Blondon AF, Balech B, Barta E, Ying Shi Chua P, Di Cola V, Heil KF, Hughes GM, Jermiin LS, Kala&#353; M, Lanfear J, Pafilis E, Palagi PM, Papageorgiou AC, Paup&#233;rio J, Psomopoulos F, Raes N, Burgin J, Gabald&#243;n T., </span><span class="title"> "The ELIXIR Biodiversity Community: Understanding short- and long-term changes in biodiversity.", </span><span><b> 12 </b></span><span> (2023): </span><span><i> F1000Research, </i></span><span class="page_info">PI ELIXIR-499, </span><a href="https://doi.org/10.12688/f1000research.133724.2"> DOI: 10.12688/f1000research.133724.2, </a><span class="abstract_text" title="Biodiversity loss is now recognised as one of the major challenges for humankind to address over the next few decades. Unless major actions are taken, the sixth mass extinction will lead to catastrophic effects on the Earth's biosphere and human health and well-being. ELIXIR can help address the technical challenges of biodiversity science, through leveraging its suite of services and expertise to enable data management and analysis activities that enhance our understanding of life on Earth and facilitate biodiversity preservation and restoration. This white paper, prepared by the ELIXIR Biodiversity Community, summarises the current status and responses, and presents a set of plans, both technical and community-oriented, that should both enhance how ELIXIR Services are applied in the biodiversity field and how ELIXIR builds connections across the many other infrastructures active in this area. We discuss the areas of highest priority, how they can be implemented in cooperation with the ELIXIR Platforms, and their connections to existing ELIXIR Communities and international consortia. The article provides a preliminary blueprint for a Biodiversity Community in ELIXIR and is an appeal to identify and involve new stakeholders."> Abstract: 184 words, </span></li><li><span class="pmcid"> PMC4804633, </span><span class="author_string"> Sarntivijai S, Vasant D, Jupp S, Saunders G, Bento AP, Gonzalez D, Betts J, Hasan S, Koscielny G, Dunham I, Parkinson H, Malone J., </span><span class="title"> "Linking rare and common disease: mapping clinical disease-phenotypes to ontologies in therapeutic target validation.", </span><span><b> 7 </b></span><span> (2016): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 8, </span><a href="https://doi.org/10.1186/s13326-016-0051-7"> DOI: 10.1186/s13326-016-0051-7, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;The Centre for Therapeutic Target Validation (CTTV - https://www.targetvalidation.org/) was established to generate therapeutic target evidence from genome-scale experiments and analyses. CTTV aims to support the validity of therapeutic targets by integrating existing and newly-generated data. Data integration has been achieved in some resources by mapping metadata such as disease and phenotypes to the Experimental Factor Ontology (EFO). Additionally, the relationship between ontology descriptions of rare and common diseases and their phenotypes can offer insights into shared biological mechanisms and potential drug targets. Ontologies are not ideal for representing the sometimes associated type relationship required. This work addresses two challenges; annotation of diverse big data, and representation of complex, sometimes associated relationships between concepts.&lt;h4&gt;Methods&lt;/h4&gt;Semantic mapping uses a combination of custom scripting, our annotation tool 'Zooma', and expert curation. Disease-phenotype associations were generated using literature mining on Europe PubMed Central abstracts, which were manually verified by experts for validity. Representation of the disease-phenotype association was achieved by the Ontology of Biomedical AssociatioN (OBAN), a generic association representation model. OBAN represents associations between a subject and object i.e., disease and its associated phenotypes and the source of evidence for that association. The indirect disease-to-disease associations are exposed through shared phenotypes. This was applied to the use case of linking rare to common diseases at the CTTV.&lt;h4&gt;Results&lt;/h4&gt;EFO yields an average of over 80% of mapping coverage in all data sources. A 42% precision is obtained from the manual verification of the text-mined disease-phenotype associations. This results in 1452 and 2810 disease-phenotype pairs for IBD and autoimmune disease and contributes towards 11,338 rare diseases associations (merged with existing published work [Am J Hum Genet 97:111-24, 2015]). An OBAN result file is downloadable at http://sourceforge.net/p/efo/code/HEAD/tree/trunk/src/efoassociations/. Twenty common diseases are linked to 85 rare diseases by shared phenotypes. A generalizable OBAN model for association representation is presented in this study.&lt;h4&gt;Conclusions&lt;/h4&gt;Here we present solutions to large-scale annotation-ontology mapping in the CTTV knowledge base, a process for disease-phenotype mining, and propose a generic association model, 'OBAN', as a means to integrate disease using shared phenotypes.&lt;h4&gt;Availability&lt;/h4&gt;EFO is released monthly and available for download at http://www.ebi.ac.uk/efo/."> Abstract: 346 words, </span></li><li><span class="pmcid"> PMC4177597, </span><span class="author_string"> Hettne KM, Dharuri H, Zhao J, Wolstencroft K, Belhajjame K, Soiland-Reyes S, Mina E, Thompson M, Cruickshank D, Verdes-Montenegro L, Garrido J, de Roure D, Corcho O, Klyne G, van Schouwen R, 't Hoen PA, Bechhofer S, Goble C, Roos M., </span><span class="title"> "Structuring research methods and data with the research object model: genomics workflows as a case study.", </span><span><b> 5(1) </b></span><span> (2014): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 41, </span><a href="https://doi.org/10.1186/2041-1480-5-41"> DOI: 10.1186/2041-1480-5-41, </a><span class="abstract_text" title='&lt;h4&gt;Background&lt;/h4&gt;One of the main challenges for biomedical research lies in the computer-assisted integrative study of large and increasingly complex combinations of data in order to understand molecular mechanisms. The preservation of the materials and methods of such computational experiments with clear annotations is essential for understanding an experiment, and this is increasingly recognized in the bioinformatics community. Our assumption is that offering means of digital, structured aggregation and annotation of the objects of an experiment will provide necessary meta-data for a scientist to understand and recreate the results of an experiment. To support this we explored a model for the semantic description of a workflow-centric Research Object (RO), where an RO is defined as a resource that aggregates other resources, e.g., datasets, software, spreadsheets, text, etc. We applied this model to a case study where we analysed human metabolite variation by workflows.&lt;h4&gt;Results&lt;/h4&gt;We present the application of the workflow-centric RO model for our bioinformatics case study. Three workflows were produced following recently defined Best Practices for workflow design. By modelling the experiment as an RO, we were able to automatically query the experiment and answer questions such as "which particular data was input to a particular workflow to test a particular hypothesis?", and "which particular conclusions were drawn from a particular workflow?".&lt;h4&gt;Conclusions&lt;/h4&gt;Applying a workflow-centric RO model to aggregate and annotate the resources used in a bioinformatics experiment, allowed us to retrieve the conclusions of the experiment in the context of the driving hypothesis, the executed workflows and their input data. The RO model is an extendable reference model that can be used by other systems as well.&lt;h4&gt;Availability&lt;/h4&gt;The Research Object is available at http://www.myexperiment.org/packs/428 The Wf4Ever Research Object Model is available at http://wf4ever.github.io/ro.'> Abstract: 281 words, </span></li><li><span class="pmcid"> PMC8075073, </span><span class="author_string"> Ammar N, Bailey JE, Davis RL, Shaban-Nejad A., </span><span class="title"> "Using a Personal Health Library-Enabled mHealth Recommender System for Self-Management of Diabetes Among Underserved Populations: Use Case for Knowledge Graphs and Linked Data.", </span><span><b> 5(3) </b></span><span> (2021): </span><span><i> JMIR formative research, </i></span><span class="page_info">PI e24738, </span><a href="https://doi.org/10.2196/24738"> DOI: 10.2196/24738, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Traditionally, digital health data management has been based on electronic health record (EHR) systems and has been handled primarily by centralized health providers. New mechanisms are needed to give patients more control over their digital health data. Personal health libraries (PHLs) provide a single point of secure access to patients' digital health data and enable the integration of knowledge stored in their digital health profiles with other sources of global knowledge. PHLs can help empower caregivers and health care providers to make informed decisions about patients' health by understanding medical events in the context of their lives.&lt;h4&gt;Objective&lt;/h4&gt;This paper reports the implementation of a mobile health digital intervention that incorporates both digital health data stored in patients' PHLs and other sources of contextual knowledge to deliver tailored recommendations for improving self-care behaviors in diabetic adults.&lt;h4&gt;Methods&lt;/h4&gt;We conducted a thematic assessment of patient functional and nonfunctional requirements that are missing from current EHRs based on evidence from the literature. We used the results to identify the technologies needed to address those requirements. We describe the technological infrastructures used to construct, manage, and integrate the types of knowledge stored in the PHL. We leverage the Social Linked Data (Solid) platform to design a fully decentralized and privacy-aware platform that supports interoperability and care integration. We provided an initial prototype design of a PHL and drafted a use case scenario that involves four actors to demonstrate how the proposed prototype can be used to address user requirements, including the construction and management of the PHL and its utilization for developing a mobile app that queries the knowledge stored and integrated into the PHL in a private and fully decentralized manner to provide better recommendations.&lt;h4&gt;Results&lt;/h4&gt;To showcase the main features of the mobile health app and the PHL, we mapped those features onto a framework comprising the user requirements identified in a use case scenario that features a preventive intervention from the diabetes self-management domain. Ongoing development of the app requires a formative evaluation study and a clinical trial to assess the impact of the digital intervention on patient-users. We provide synopses of both study protocols.&lt;h4&gt;Conclusions&lt;/h4&gt;The proposed PHL helps patients and their caregivers take a central role in making decisions regarding their health and equips their health care providers with informatics tools that support the collection and interpretation of the collected knowledge. By exposing the PHL functionality as an open service, we foster the development of third-party applications or services and provide motivational technological support in several projects crossing different domains of interest."> Abstract: 416 words, </span></li><li><span class="pmcid"> PMC4130157, </span><span class="author_string"> Oetting WS, Robinson PN, Greenblatt MS, Cotton RG, Beck T, Carey JC, Doelken SC, Girdea M, Groza T, Hamilton CM, Hamosh A, Kerner B, MacArthur JA, Maglott DR, Mons B, Rehm HL, Schofield PN, Searle BA, Smedley D, Smith CL, Bernstein IT, Zankl A, Zhao EY., </span><span class="title"> "Getting ready for the Human Phenome Project: the 2012 forum of the Human Variome Project.", </span><span><b> 34(4) </b></span><span> (2013): </span><span><i> Human mutation, </i></span><span class="page_info">PI 661-666, </span><a href="https://doi.org/10.1002/humu.22293"> DOI: 10.1002/humu.22293, </a><span class="abstract_text" title="A forum of the Human Variome Project (HVP) was held as a satellite to the 2012 Annual Meeting of the American Society of Human Genetics in San Francisco, California. The theme of this meeting was &quot;Getting Ready for the Human Phenome Project.&quot; Understanding the genetic contribution to both rare single-gene &quot;Mendelian&quot; disorders and more complex common diseases will require integration of research efforts among many fields and better defined phenotypes. The HVP is dedicated to bringing together researchers and research populations throughout the world to provide the resources to investigate the impact of genetic variation on disease. To this end, there needs to be a greater sharing of phenotype and genotype data. For this to occur, many databases that currently exist will need to become interoperable to allow for the combining of cohorts with similar phenotypes to increase statistical power for studies attempting to identify novel disease genes or causative genetic variants. Improved systems and tools that enhance the collection of phenotype data from clinicians are urgently needed. This meeting begins the HVP's effort toward this important goal."> Abstract: 178 words, </span></li><li><span class="pmcid"> PMC7590901, </span><span class="author_string"> Holinski A, Burke ML, Morgan SL, McQuilton P, Palagi PM., </span><span class="title"> "Biocuration - mapping resources and needs.", </span><span><b> 9 </b></span><span> (2020): </span><span><i> F1000Research, </i></span><span class="page_info">PI ELIXIR-1094, </span><a href="https://doi.org/10.12688/f1000research.25413.2"> DOI: 10.12688/f1000research.25413.2, </a><span class="abstract_text" title="&lt;b&gt;Background:&lt;/b&gt; Biocuration involves a variety of teams and individuals across the globe.&#160;However, they may not self-identify as biocurators, as they may be unaware of biocuration as a career path or because biocuration is only part of their role. The lack of a clear, up-to-date profile of biocuration creates challenges for organisations like ELIXIR, the ISB and GOBLET to systematically support biocurators and for biocurators themselves to develop their own careers. Therefore, the ELIXIR Training Platform launched an Implementation Study in order to i) identify communities of biocurators, ii) map the type of curation work being done, iii) assess biocuration training, and iv) draw a picture of biocuration career development. &lt;b&gt;Methods:&lt;/b&gt; To achieve the goals of the study, we carried out a global survey on the nature of biocuration work, the tools and resources that are used, training that has been received and additional training needs. To examine these topics in more detail we ran workshop-based discussions at ISB Biocuration Conference 2019 and the ELIXIR All Hands Meeting 2019. We also had guided conversations with selected people from the EMBL-European Bioinformatics Institute. &lt;b&gt;Results:&lt;/b&gt; The study illustrates that biocurators have diverse job titles, are highly skilled, perform a variety of activities and use a wide range of tools and resources. The study emphasises the need for training in programming and coding skills, but also highlights the difficulties curators face in terms of career development and community building. &lt;b&gt;Conclusion:&lt;/b&gt; Biocurators themselves, as well as organisations like ELIXIR, GOBLET and ISB must work together towards structural change to overcome these difficulties. In this article we discuss recommendations to ensure that biocuration as a role is visible and valued, thereby helping biocurators to proceed with their career."> Abstract: 283 words, </span></li><li><span class="pmcid"> PMC9278328, </span><span class="author_string"> Cui H, Ford B, Starr J, Reznicek A, Zhang L, Macklin JA., </span><span class="title"> "Authors' attitude toward adopting a new workflow to improve the computability of phenotype publications.", </span><span><b> 2022 </b></span><span> (2022): </span><span><i> Database : the journal of biological databases and curation, </i></span><span class="page_info">PI baac001, </span><a href="https://doi.org/10.1093/database/baac001"> DOI: 10.1093/database/baac001, </a><span class="abstract_text" title="Critical to answering large-scale questions in biology is the integration of knowledge from different disciplines into a coherent, computable whole. Controlled vocabularies such as ontologies represent a clear path toward this goal. Using survey questionnaires, we examined the attitudes of biologists toward adopting controlled vocabularies in phenotype publications. Our questions cover current experience and overall attitude with controlled vocabularies, the awareness of the issues around ambiguity and inconsistency in phenotype descriptions and post-publication professional data curation, the preferred solutions and the effort and desired rewards for adopting a new authoring workflow. Results suggest that although the existence of controlled vocabularies is widespread, their use is not common. A majority of respondents (74%) are frustrated with ambiguity in phenotypic descriptions, and there is a strong agreement (mean agreement score 4.21 out of 5) that author curation would better reflect the original meaning of phenotype data. Moreover, the vast majority (85%) of researchers would try a new authoring workflow if resultant data were more consistent and less ambiguous. Even more respondents (93%) suggested that they would try and possibly adopt a new authoring workflow if it required 5% additional effort as compared to normal, but higher rates resulted in a steep decline in likely adoption rates. Among the four different types of rewards, two types of citations were the most desired incentives for authors to produce computable data. Overall, our results suggest the adoption of a new authoring workflow would be accelerated by a user-friendly and efficient software-authoring tool, an increased awareness of the challenges text ambiguity creates for external curators and an elevated appreciation of the benefits of controlled vocabularies."> Abstract: 269 words, </span></li><li><span class="pmcid"> PMC5210543, </span><span class="author_string"> Koscielny G, An P, Carvalho-Silva D, Cham JA, Fumis L, Gasparyan R, Hasan S, Karamanis N, Maguire M, Papa E, Pierleoni A, Pignatelli M, Platt T, Rowland F, Wankar P, Bento AP, Burdett T, Fabregat A, Forbes S, Gaulton A, Gonzalez CY, Hermjakob H, Hersey A, Jupe S, Kafkas &#350;, Keays M, Leroy C, Lopez FJ, Magarinos MP, Malone J, McEntyre J, Munoz-Pomer Fuentes A, O'Donovan C, Papatheodorou I, Parkinson H, Palka B, Paschall J, Petryszak R, Pratanwanich N, Sarntivijal S, Saunders G, Sidiropoulos K, Smith T, Sondka Z, Stegle O, Tang YA, Turner E, Vaughan B, Vrousgou O, Watkins X, Martin MJ, Sanseau P, Vamathevan J, Birney E, Barrett J, Dunham I., </span><span class="title"> "Open Targets: a platform for therapeutic target identification and validation.", </span><span><b> 45(D1) </b></span><span> (2017): </span><span><i> Nucleic acids research, </i></span><span class="page_info">PI D985-D994, </span><a href="https://doi.org/10.1093/nar/gkw1055"> DOI: 10.1093/nar/gkw1055, </a><span class="abstract_text" title="We have designed and developed a data integration and visualization platform that provides evidence about the association of known and potential drug targets with diseases. The platform is designed to support identification and prioritization of biological targets for follow-up. Each drug target is linked to a disease using integrated genome-wide data from a broad range of data sources. The platform provides either a target-centric workflow to identify diseases that may be associated with a specific target, or a disease-centric workflow to identify targets that may be associated with a specific disease. Users can easily transition between these target- and disease-centric workflows. The Open Targets Validation Platform is accessible at https://www.targetvalidation.org."> Abstract: 110 words, </span></li><li><span class="pmcid"> PMC2868544, </span><span class="author_string"> Rinaldi A., </span><span class="title"> "For I dipped into the future.", </span><span><b> 11(5) </b></span><span> (2010): </span><span><i> EMBO reports, </i></span><span class="page_info">PI 345-349, </span><a href="https://doi.org/10.1038/embor.2010.57"> DOI: 10.1038/embor.2010.57, </a></li><li><span class="pmcid"> PMC4474486, </span><span class="author_string"> Burns GA, Turner JA., </span><span class="title"> "Modeling functional Magnetic Resonance Imaging (fMRI) experimental variables in the Ontology of Experimental Variables and Values (OoEVV).", </span><span><b> 82 </b></span><span> (2013): </span><span><i> NeuroImage, </i></span><span class="page_info">PI 662-670, </span><a href="https://doi.org/10.1016/j.neuroimage.2013.05.024"> DOI: 10.1016/j.neuroimage.2013.05.024, </a><span class="abstract_text" title="Neuroimaging data is raw material for cognitive neuroscience experiments, leading to scientific knowledge about human neurological and psychological disease, language, perception, attention and ultimately, cognition. The structure of the variables used in the experimental design defines the structure of the data gathered in the experiments; this in turn structures the interpretative assertions that may be presented as experimental conclusions. Representing these assertions and the experimental data which support them in a computable way means that they could be used in logical reasoning environments, i.e. for automated meta-analyses, or linking hypotheses and results across different levels of neuroscientific experiments. Therefore, a crucial first step in being able to represent neuroimaging results in a clear, computable way is to develop representations for the scientific variables involved in neuroimaging experiments. These representations should be expressive, computable, valid, extensible, and easy-to-use. They should also leverage existing semantic standards to interoperate easily with other systems. We present an ontology design pattern called the Ontology of Experimental Variables and Values (OoEVV). This is designed to provide a lightweight framework to capture mathematical properties of data, with appropriate 'hooks' to permit linkage to other ontology-driven projects (such as the Ontology of Biomedical Investigations, OBI). We instantiate the OoEVV system with a small number of functional Magnetic Resonance Imaging datasets, to demonstrate the system's ability to describe the variables of a neuroimaging experiment. OoEVV is designed to be compatible with the XCEDE neuroimaging data standard for data collection terminology, and with the Cognitive Paradigm Ontology (CogPO) for specific reasoning elements of neuroimaging experimental designs."> Abstract: 256 words, </span></li><li><span class="pmcid"> PMC5385323, </span><span class="author_string"> Goldmann D, Zdrazil B, Digles D, Ecker GF., </span><span class="title"> "Empowering pharmacoinformatics by linked life science data.", </span><span><b> 31(3) </b></span><span> (2017): </span><span><i> Journal of computer-aided molecular design, </i></span><span class="page_info">PI 319-328, </span><a href="https://doi.org/10.1007/s10822-016-9990-4"> DOI: 10.1007/s10822-016-9990-4, </a><span class="abstract_text" title="With the public availability of large data sources such as ChEMBLdb and the Open PHACTS Discovery Platform, retrieval of data sets for certain protein targets of interest with consistent assay conditions is no longer a time consuming process. Especially the use of workflow engines such as KNIME or Pipeline Pilot allows complex queries and enables to simultaneously search for several targets. Data can then directly be used as input to various ligand- and structure-based studies. In this contribution, using in-house projects on P-gp inhibition, transporter selectivity, and TRPV1 modulation we outline how the incorporation of linked life science data in the daily execution of projects allowed to expand our approaches from conventional Hansch analysis to complex, integrated multilayer models."> Abstract: 119 words, </span></li><li><span class="pmcid"> PMC11181106, </span><span class="author_string"> K&#246;hler CA, Ulianych D, Gr&#252;n S, Decker S, Denker M., </span><span class="title"> "Facilitating the Sharing of Electrophysiology Data Analysis Results Through In-Depth Provenance Capture.", </span><span><b> 11(6) </b></span><span> (2024): </span><span><i> eNeuro, </i></span><span class="page_info">PI ENEURO.0476-23.2024, </span><a href="https://doi.org/10.1523/eneuro.0476-23.2024"> DOI: 10.1523/eneuro.0476-23.2024, </a><span class="abstract_text" title="Scientific research demands reproducibility and transparency, particularly in data-intensive fields like electrophysiology. Electrophysiology data are typically analyzed using scripts that generate output files, including figures. Handling these results poses several challenges due to the complexity and iterative nature of the analysis process. These stem from the difficulty to discern the analysis steps, parameters, and data flow from the results, making knowledge transfer and findability challenging in collaborative settings. Provenance information tracks data lineage and processes applied to it, and provenance capture during the execution of an analysis script can address those challenges. We present Alpaca (Automated Lightweight Provenance Capture), a tool that captures fine-grained provenance information with minimal user intervention when running data analysis pipelines implemented in Python scripts. Alpaca records inputs, outputs, and function parameters and structures information according to the W3C PROV standard. We demonstrate the tool using a realistic use case involving multichannel local field potential recordings of a neurophysiological experiment, highlighting how the tool makes result details known in a standardized manner in order to address the challenges of the analysis process. Ultimately, using Alpaca will help to represent results according to the FAIR principles, which will improve research reproducibility and facilitate sharing the results of data analyses."> Abstract: 202 words, </span></li><li><span class="pmcid"> PMC4129183, </span><span class="author_string"> Livingston KM, Bada M, Hunter LE, Verspoor K., </span><span class="title"> "Representing annotation compositionality and provenance for the Semantic Web.", </span><span><b> 4 </b></span><span> (2013): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 38, </span><a href="https://doi.org/10.1186/2041-1480-4-38"> DOI: 10.1186/2041-1480-4-38, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Though the annotation of digital artifacts with metadata has a long history, the bulk of that work focuses on the association of single terms or concepts to single targets. As annotation efforts expand to capture more complex information, annotations will need to be able to refer to knowledge structures formally defined in terms of more atomic knowledge structures. Existing provenance efforts in the Semantic Web domain primarily focus on tracking provenance at the level of whole triples and do not provide enough detail to track how individual triple elements of annotations were derived from triple elements of other annotations.&lt;h4&gt;Results&lt;/h4&gt;We present a task- and domain-independent ontological model for capturing annotations and their linkage to their denoted knowledge representations, which can be singular concepts or more complex sets of assertions. We have implemented this model as an extension of the Information Artifact Ontology in OWL and made it freely available, and we show how it can be integrated with several prominent annotation and provenance models. We present several application areas for the model, ranging from linguistic annotation of text to the annotation of disease-associations in genome sequences.&lt;h4&gt;Conclusions&lt;/h4&gt;With this model, progressively more complex annotations can be composed from other annotations, and the provenance of compositional annotations can be represented at the annotation level or at the level of individual elements of the RDF triples composing the annotations. This in turn allows for progressively richer annotations to be constructed from previous annotation efforts, the precise provenance recording of which facilitates evidence-based inference and error tracking."> Abstract: 251 words, </span></li><li><span class="pmcid"> PMC8042632, </span><span class="author_string"> Masara B, van der Poll JA, Maaza M., </span><span class="title"> "A nanotechnology-foresight perspective of South Africa.", </span><span><b> 23(4) </b></span><span> (2021): </span><span><i> Journal of nanoparticle research : an interdisciplinary forum for nanoscale science and technology, </i></span><span class="page_info">PI 92, </span><a href="https://doi.org/10.1007/s11051-021-05193-6"> DOI: 10.1007/s11051-021-05193-6, </a><span class="abstract_text" title="This paper presents a foresight perspective of nanotechnology in South Africa based on a 20-year period scientometric analysis of the country's nanotechnology&#160;publications on the Web of Science (WoS) Core Collection. Firstly, publication trends are reported; then, possible socio-economic relevant sectors arising from this information are determined. Lastly, indicators that can be used in foresight exercises to evaluate the potential nanotechnology research areas in South Africa are examined. The 20-year review is also compared with the recent past year, 2019, to identify any changing trends. South Africa's nanotechnology publications per year grew exponentially from 68 papers in 2000 to 1672 in 2019, an increase of 2459%. The total share of nanotech publications increased from 1.4% in 2000 to 6.6% in 2019, thus a 0.52% increase per year. Compared with Brazil, Russia, India and China, the BRICS countries, South Africa has the lowest nanotechnology productivity with an activity index of 0.68. Over the last 5 years, South Africa nanotech publications had a Hirsch-index of 94 and an average citations rate of 12.76 per paper. Universities are the most prominent publishers, and there are very few publications from the private sector, which can negatively impact the commercialisation of nanotechnology research. The top 10 most prolific researchers, author or co-author over 20% of the nanotechnology papers&#160;are reported. A mixture of old and new top researchers' names suggests succession planning in the system as the years progress. The emergence of computer science as one of the top 20 subjects publishing in nanotech in 2019 and a high level of researcher collaboration suggests possible convergence of nanotech, information technology and artificial intelligence in South Africa. The strategic socio-economic-focused nanotechnology research areas identified for South Africa include material science, photoluminance and optics, medicine, catalysis, electronics, energy, biotech, magnetism, sensors, water and communicable diseases. The top collaborating countries, top researchers, top institutions and nanotechnology economic hubs are reported for each strategic research area. The level of innovation was evaluated using the nanotechnology value chain, and there is a meagre 3.5% of papers reporting on nano-enabled products."> Abstract: 340 words, </span></li><li><span class="pmcid"> PMC7250615, </span><span class="author_string"> Tiddi I, Balliet D, ten Teije A., </span><span class="title"> "Fostering Scientific Meta-analyses with Knowledge Graphs: A Case-Study", </span><span><b> 12123 </b></span><span> (2020): </span><span><i> The Semantic Web17th International Conference, ESWC 2020, Heraklion, Crete, Greece, May 31&#8211;June 4, 2020, Proceedings, </i></span><span class="page_info">PI 287-303, </span><span class="abstract_text" title="A meta-analysis is a Science of Science method widely used in the medical and social sciences to review, aggregate and quantitatively synthesise a body of studies that address the same research question. With the volume of research growing exponentially every year, conducting meta-analyses can be costly and inefficient, as a significant amount of time and human efforts needs to be spent in finding studies meeting research criteria, annotating them, and properly performing the statistical analyses to summarise the findings. In this work, we show these issues can be tackled with semantic representations and technologies, using a social science scenario as case-study. We show how the domain-specific content of research outputs can be represented and used to facilitate their search, analysis and synthesis. We present the very first representation of the domain of human cooperation, and the application we built on top of this to help experts in performing meta-analyses semi-automatically. Using few application scenarios, we show how our approach supports the various phases meta-analyses, and more in general contributes towards research replication and automated hypotheses generation."> Abstract: 176 words, </span></li><li><span class="pmcid"> PMC5755483, </span><span class="author_string"> Garcia A, Lopez F, Garcia L, Giraldo O, Bucheli V, Dumontier M., </span><span class="title"> "Biotea: semantics for Pubmed Central.", </span><span><b> 6 </b></span><span> (2018): </span><span><i> PeerJ, </i></span><span class="page_info">PI e4201, </span><a href="https://doi.org/10.7717/peerj.4201"> DOI: 10.7717/peerj.4201, </a><span class="abstract_text" title="A significant portion of biomedical literature is represented in a manner that makes it difficult for consumers to find or aggregate content through a computational query. One approach to facilitate reuse of the scientific literature is to structure this information as linked data using standardized web technologies. In this paper we present the second version of Biotea, a semantic, linked data version of the open-access subset of PubMed Central that has been enhanced with specialized annotation pipelines that uses existing infrastructure from the National Center for Biomedical Ontology. We expose our models, services, software and datasets. Our infrastructure enables manual and semi-automatic annotation, resulting data are represented as RDF-based linked data and can be readily queried using the SPARQL query language. We illustrate the utility of our system with several use cases. Our datasets, methods and techniques are available at http://biotea.github.io."> Abstract: 141 words, </span></li><li><span class="pmcid"> PMC2663789, </span><span class="author_string"> Shotton D, Portwin K, Klyne G, Miles A., </span><span class="title"> "Adventures in semantic publishing: exemplar semantic enhancements of a research article.", </span><span><b> 5(4) </b></span><span> (2009): </span><span><i> PLoS computational biology, </i></span><span class="page_info">PI e1000361, </span><a href="https://doi.org/10.1371/journal.pcbi.1000361"> DOI: 10.1371/journal.pcbi.1000361, </a><span class="abstract_text" title='Scientific innovation depends on finding, integrating, and re-using the products of previous research. Here we explore how recent developments in Web technology, particularly those related to the publication of data and metadata, might assist that process by providing semantic enhancements to journal articles within the mainstream process of scholarly journal publishing. We exemplify this by describing semantic enhancements we have made to a recent biomedical research article taken from PLoS Neglected Tropical Diseases, providing enrichment to its content and increased access to datasets within it. These semantic enhancements include provision of live DOIs and hyperlinks; semantic markup of textual terms, with links to relevant third-party information resources; interactive figures; a re-orderable reference list; a document summary containing a study summary, a tag cloud, and a citation analysis; and two novel types of semantic enrichment: the first, a Supporting Claims Tooltip to permit "Citations in Context", and the second, Tag Trees that bring together semantically related terms. In addition, we have published downloadable spreadsheets containing data from within tables and figures, have enriched these with provenance information, and have demonstrated various types of data fusion (mashups) with results from other research articles and with Google Maps. We have also published machine-readable RDF metadata both about the article and about the references it cites, for which we developed a Citation Typing Ontology, CiTO (http://purl.org/net/cito/). The enhanced article, which is available at http://dx.doi.org/10.1371/journal.pntd.0000228.x001, presents a compelling existence proof of the possibilities of semantic publication. We hope the showcase of examples and ideas it contains, described in this paper, will excite the imaginations of researchers and publishers, stimulating them to explore the possibilities of semantic publishing for their own research articles, and thereby break down present barriers to the discovery and re-use of information within traditional modes of scholarly communication.'> Abstract: 296 words, </span></li><li><span class="pmcid"> PMC6556902, </span><span class="author_string"> Saqi M, Lysenko A, Guo YK, Tsunoda T, Auffray C., </span><span class="title"> "Navigating the disease landscape: knowledge representations for contextualizing molecular signatures.", </span><span><b> 20(2) </b></span><span> (2019): </span><span><i> Briefings in bioinformatics, </i></span><span class="page_info">PI 609-623, </span><a href="https://doi.org/10.1093/bib/bby025"> DOI: 10.1093/bib/bby025, </a><span class="abstract_text" title="Large amounts of data emerging from experiments in molecular medicine are leading to the identification of molecular signatures associated with disease subtypes. The contextualization of these patterns is important for obtaining mechanistic insight into the aberrant processes associated with a disease, and this typically involves the integration of multiple heterogeneous types of data. In this review, we discuss knowledge representations that can be useful to explore the biological context of molecular signatures, in particular three main approaches, namely, pathway mapping approaches, molecular network centric approaches and approaches that represent biological statements as knowledge graphs. We discuss the utility of each of these paradigms, illustrate how they can be leveraged with selected practical examples and identify ongoing challenges for this field of research."> Abstract: 122 words, </span></li><li><span class="pmcid"> PMC4108886, </span><span class="author_string"> B&#246;lling C, Weidlich M, Holzh&#252;tter HG., </span><span class="title"> "SEE: structured representation of scientific evidence in the biomedical domain using Semantic Web techniques.", </span><span><b> 5(Suppl 1 Proceedings of the Bio-Ontologies Spec Interest) </b></span><span> (2014): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI S1, </span><a href="https://doi.org/10.1186/2041-1480-5-s1-s1"> DOI: 10.1186/2041-1480-5-s1-s1, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;Accounts of evidence are vital to evaluate and reproduce scientific findings and integrate data on an informed basis. Currently, such accounts are often inadequate, unstandardized and inaccessible for computational knowledge engineering even though computational technologies, among them those of the semantic web, are ever more employed to represent, disseminate and integrate biomedical data and knowledge.&lt;h4&gt;Results&lt;/h4&gt;We present SEE (Semantic EvidencE), an RDF/OWL based approach for detailed representation of evidence in terms of the argumentative structure of the supporting background for claims even in complex settings. We derive design principles and identify minimal components for the representation of evidence. We specify the Reasoning and Discourse Ontology (RDO), an OWL representation of the model of scientific claims, their subjects, their provenance and their argumentative relations underlying the SEE approach. We demonstrate the application of SEE and illustrate its design patterns in a case study by providing an expressive account of the evidence for certain claims regarding the isolation of the enzyme glutamine synthetase.&lt;h4&gt;Conclusions&lt;/h4&gt;SEE is suited to provide coherent and computationally accessible representations of evidence-related information such as the materials, methods, assumptions, reasoning and information sources used to establish a scientific finding by adopting a consistently claim-based perspective on scientific results and their evidence. SEE allows for extensible evidence representations, in which the level of detail can be adjusted and which can be extended as needed. It supports representation of arbitrary many consecutive layers of interpretation and attribution and different evaluations of the same data. SEE and its underlying model could be a valuable component in a variety of use cases that require careful representation or examination of evidence for data presented on the semantic web or in other formats."> Abstract: 276 words, </span></li><li><span class="pmcid"> PMC4294709, </span><span class="author_string"> Thompson BA, Spurdle AB, Plazzer JP, Greenblatt MS, Akagi K, Al-Mulla F, Bapat B, Bernstein I, Capell&#225; G, den Dunnen JT, du Sart D, Fabre A, Farrell MP, Farrington SM, Frayling IM, Frebourg T, Goldgar DE, Heinen CD, Holinski-Feder E, Kohonen-Corish M, Robinson KL, Leung SY, Martins A, Moller P, Morak M, Nystrom M, Peltomaki P, Pineda M, Qi M, Ramesar R, Rasmussen LJ, Royer-Pokora B, Scott RJ, Sijmons R, Tavtigian SV, Tops CM, Weber T, Wijnen J, Woods MO, Macrae F, Genuardi M., </span><span class="title"> "Application of a 5-tiered scheme for standardized classification of 2,360 unique mismatch repair gene variants in the InSiGHT locus-specific database.", </span><span><b> 46(2) </b></span><span> (2014): </span><span><i> Nature genetics, </i></span><span class="page_info">PI 107-115, </span><a href="https://doi.org/10.1038/ng.2854"> DOI: 10.1038/ng.2854, </a><span class="abstract_text" title="The clinical classification of hereditary sequence variants identified in disease-related genes directly affects clinical management of patients and their relatives. The International Society for Gastrointestinal Hereditary Tumours (InSiGHT) undertook a collaborative effort to develop, test and apply a standardized classification scheme to constitutional variants in the Lynch syndrome-associated genes MLH1, MSH2, MSH6 and PMS2. Unpublished data submission was encouraged to assist in variant classification and was recognized through microattribution. The scheme was refined by multidisciplinary expert committee review of the clinical and functional data available for variants, applied to 2,360 sequence alterations, and disseminated online. Assessment using validated criteria altered classifications for 66% of 12,006 database entries. Clinical recommendations based on transparent evaluation are now possible for 1,370 variants that were not obviously protein truncating from nomenclature. This large-scale endeavor will facilitate the consistent management of families suspected to have Lynch syndrome and demonstrates the value of multidisciplinary collaboration in the curation and classification of variants in public locus-specific databases."> Abstract: 160 words, </span></li><li><span class="pmcid"> PMC2895735, </span><span class="author_string"> Rebholz-Schuhmann D, Nenadic G., </span><span class="title"> "Biomedical semantics: the hub for biomedical research 2.0.", </span><span><b> 1(1) </b></span><span> (2010): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 1, </span><a href="https://doi.org/10.1186/2041-1480-1-1"> DOI: 10.1186/2041-1480-1-1, </a></li><li><span class="pmcid"> PMC7141167, </span><span class="author_string"> Vos RA, Katayama T, Mishima H, Kawano S, Kawashima S, Kim JD, Moriya Y, Tokimatsu T, Yamaguchi A, Yamamoto Y, Wu H, Amstutz P, Antezana E, Aoki NP, Arakawa K, Bolleman JT, Bolton E, Bonnal RJP, Bono H, Burger K, Chiba H, Cohen KB, Deutsch EW, Fern&#225;ndez-Breis JT, Fu G, Fujisawa T, Fukushima A, Garc&#237;a A, Goto N, Groza T, Hercus C, Hoehndorf R, Itaya K, Juty N, Kawashima T, Kim JH, Kinjo AR, Kotera M, Kozaki K, Kumagai S, Kushida T, L&#252;tteke T, Matsubara M, Miyamoto J, Mohsen A, Mori H, Naito Y, Nakazato T, Nguyen-Xuan J, Nishida K, Nishida N, Nishide H, Ogishima S, Ohta T, Okuda S, Paten B, Perret JL, Prathipati P, Prins P, Queralt-Rosinach N, Shinmachi D, Suzuki S, Tabata T, Takatsuki T, Taylor K, Thompson M, Uchiyama I, Vieira B, Wei CH, Wilkinson M, Yamada I, Yamanaka R, Yoshitake K, Yoshizawa AC, Dumontier M, Kosaki K, Takagi T., </span><span class="title"> "BioHackathon 2015: Semantics of data for life sciences and reproducible research.", </span><span><b> 9 </b></span><span> (2020): </span><span><i> F1000Research, </i></span><span class="page_info">PI 136, </span><a href="https://doi.org/10.12688/f1000research.18236.1"> DOI: 10.12688/f1000research.18236.1, </a><span class="abstract_text" title="We report on the activities of the 2015 edition of the BioHackathon, an annual event that brings together researchers and developers from around the world to develop tools and technologies that promote the reusability of biological data. We discuss issues surrounding the representation, publication, integration, mining and reuse of biological data and metadata across a wide range of biomedical data types of relevance for the life sciences, including chemistry, genotypes and phenotypes, orthology and phylogeny, proteomics, genomics, glycomics, and metabolomics. We describe our progress to address ongoing challenges to the reusability and reproducibility of research results, and identify outstanding issues that continue to impede the progress of bioinformatics research. We share our perspective on the state of the art, continued challenges, and goals for future research and development for the life sciences Semantic Web."> Abstract: 134 words, </span></li><li><span class="pmcid"> PMC6433895, </span><span class="author_string"> Neal ML, K&#246;nig M, Nickerson D, M&#305;s&#305;rl&#305; G, Kalbasi R, Dr&#228;ger A, Atalag K, Chelliah V, Cooling MT, Cook DL, Crook S, de Alba M, Friedman SH, Garny A, Gennari JH, Gleeson P, Golebiewski M, Hucka M, Juty N, Myers C, Olivier BG, Sauro HM, Scharm M, Snoep JL, Tour&#233; V, Wipat A, Wolkenhauer O, Waltemath D., </span><span class="title"> "Harmonizing semantic annotations for computational models in biology.", </span><span><b> 20(2) </b></span><span> (2019): </span><span><i> Briefings in bioinformatics, </i></span><span class="page_info">PI 540-550, </span><a href="https://doi.org/10.1093/bib/bby087"> DOI: 10.1093/bib/bby087, </a><span class="abstract_text" title="Life science researchers use computational models to articulate and test hypotheses about the behavior of biological systems. Semantic annotation is a critical component for enhancing the interoperability and reusability of such models as well as for the integration of the data needed for model parameterization and validation. Encoded as machine-readable links to knowledge resource terms, semantic annotations describe the computational or biological meaning of what models and data represent. These annotations help researchers find and repurpose models, accelerate model composition and enable knowledge integration across model repositories and experimental data stores. However, realizing the potential benefits of semantic annotation requires the development of model annotation standards that adhere to a community-based annotation protocol. Without such standards, tool developers must account for a variety of annotation formats and approaches, a situation that can become prohibitively cumbersome and which can defeat the purpose of linking model elements to controlled knowledge resource terms. Currently, no consensus protocol for semantic annotation exists among the larger biological modeling community. Here, we report on the landscape of current annotation practices among the COmputational Modeling in BIology NEtwork community and provide a set of recommendations for building a consensus approach to semantic annotation."> Abstract: 196 words, </span></li><li><span class="pmcid"> PMC4015691, </span><span class="author_string"> Dumontier M, Baker CJ, Baran J, Callahan A, Chepelev L, Cruz-Toledo J, Del Rio NR, Duck G, Furlong LI, Keath N, Klassen D, McCusker JP, Queralt-Rosinach N, Samwald M, Villanueva-Rosales N, Wilkinson MD, Hoehndorf R., </span><span class="title"> "The Semanticscience Integrated Ontology (SIO) for biomedical research and knowledge discovery.", </span><span><b> 5(1) </b></span><span> (2014): </span><span><i> Journal of biomedical semantics, </i></span><span class="page_info">PI 14, </span><a href="https://doi.org/10.1186/2041-1480-5-14"> DOI: 10.1186/2041-1480-5-14, </a><span class="abstract_text" title="The Semanticscience Integrated Ontology (SIO) is an ontology to facilitate biomedical knowledge discovery. SIO features a simple upper level comprised of essential types and relations for the rich description of arbitrary (real, hypothesized, virtual, fictional) objects, processes and their attributes. SIO specifies simple design patterns to describe and associate qualities, capabilities, functions, quantities, and informational entities including textual, geometrical, and mathematical entities, and provides specific extensions in the domains of chemistry, biology, biochemistry, and bioinformatics. SIO provides an ontological foundation for the Bio2RDF linked data for the life sciences project and is used for semantic integration and discovery for SADI-based semantic web services. SIO is freely available to all users under a creative commons by attribution license. See website for further information: http://sio.semanticscience.org."> Abstract: 123 words, </span></li><li><span class="pmcid"> PMC3503260, </span><span class="author_string"> Ekins S, Clark AM, Williams AJ., </span><span class="title"> "Open Drug Discovery Teams: A Chemistry Mobile App for Collaboration.", </span><span><b> 31(8) </b></span><span> (2012): </span><span><i> Molecular informatics, </i></span><span class="page_info">PI 585-597, </span><a href="https://doi.org/10.1002/minf.201200034"> DOI: 10.1002/minf.201200034, </a><span class="abstract_text" title="The Open Drug Discovery Teams (ODDT) project provides a mobile app primarily intended as a research topic aggregator of predominantly open science data collected from various sources on the internet. It exists to facilitate interdisciplinary teamwork and to relieve the user from data overload, delivering access to information that is highly relevant and focused on their topic areas of interest. Research topics include areas of chemistry and adjacent molecule-oriented biomedical sciences, with an emphasis on those which are most amenable to open research at present. These include rare and neglected diseases, and precompetitive and public-good initiatives such as green chemistry. The ODDT project uses a free mobile app as user entry point. The app has a magazine-like interface, and server-side infrastructure for hosting chemistry-related data as well as value added services. The project is open to participation from anyone and provides the ability for users to make annotations and assertions, thereby contributing to the collective value of the data to the engaged community. Much of the content is derived from public sources, but the platform is also amenable to commercial data input. The technology could also be readily used in-house by organizations as a research aggregator that could integrate internal and external science and discussion. The infrastructure for the app is currently based upon the Twitter API as a useful proof of concept for a real time source of publicly generated content. This could be extended further by accessing other APIs providing news and data feeds of relevance to a particular area of interest. As the project evolves, social networking features will be developed for organizing participants into teams, with various forms of communication and content management possible."> Abstract: 278 words, </span></li><li><span class="pmcid"> PMC3176268, </span><span class="author_string"> Russ TA, Ramakrishnan C, Hovy EH, Bota M, Burns GA., </span><span class="title"> "Knowledge engineering tools for reasoning with scientific observations and interpretations: a neural connectivity use case.", </span><span><b> 12 </b></span><span> (2011): </span><span><i> BMC bioinformatics, </i></span><span class="page_info">PI 351, </span><a href="https://doi.org/10.1186/1471-2105-12-351"> DOI: 10.1186/1471-2105-12-351, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;We address the goal of curating observations from published experiments in a generalizable form; reasoning over these observations to generate interpretations and then querying this interpreted knowledge to supply the supporting evidence. We present web-application software as part of the 'BioScholar' project (R01-GM083871) that fully instantiates this process for a well-defined domain: using tract-tracing experiments to study the neural connectivity of the rat brain.&lt;h4&gt;Results&lt;/h4&gt;The main contribution of this work is to provide the first instantiation of a knowledge representation for experimental observations called 'Knowledge Engineering from Experimental Design' (KEfED) based on experimental variables and their interdependencies. The software has three parts: (a) the KEfED model editor - a design editor for creating KEfED models by drawing a flow diagram of an experimental protocol; (b) the KEfED data interface - a spreadsheet-like tool that permits users to enter experimental data pertaining to a specific model; (c) a 'neural connection matrix' interface that presents neural connectivity as a table of ordinal connection strengths representing the interpretations of tract-tracing data. This tool also allows the user to view experimental evidence pertaining to a specific connection. BioScholar is built in Flex 3.5. It uses Persevere (a noSQL database) as a flexible data store and PowerLoom&#174; (a mature First Order Logic reasoning system) to execute queries using spatial reasoning over the BAMS neuroanatomical ontology.&lt;h4&gt;Conclusions&lt;/h4&gt;We first introduce the KEfED approach as a general approach and describe its possible role as a way of introducing structured reasoning into models of argumentation within new models of scientific publication. We then describe the design and implementation of our example application: the BioScholar software. This is presented as a possible biocuration interface and supplementary reasoning toolkit for a larger, more specialized bioinformatics system: the Brain Architecture Management System (BAMS)."> Abstract: 289 words, </span></li><li><span class="pmcid"> PMC4514623, </span><span class="author_string"> Read KB, Sheehan JR, Huerta MF, Knecht LS, Mork JG, Humphreys BL, NIH Big Data Annotator Group., </span><span class="title"> "Sizing the Problem of Improving Discovery and Access to NIH-Funded Data: A Preliminary Study.", </span><span><b> 10(7) </b></span><span> (2015): </span><span><i> PloS one, </i></span><span class="page_info">PI e0132735, </span><a href="https://doi.org/10.1371/journal.pone.0132735"> DOI: 10.1371/journal.pone.0132735, </a><span class="abstract_text" title='&lt;h4&gt;Objective&lt;/h4&gt;This study informs efforts to improve the discoverability of and access to biomedical datasets by providing a preliminary estimate of the number and type of datasets generated annually by research funded by the U.S. National Institutes of Health (NIH). It focuses on those datasets that are "invisible" or not deposited in a known repository.&lt;h4&gt;Methods&lt;/h4&gt;We analyzed NIH-funded journal articles that were published in 2011, cited in PubMed and deposited in PubMed Central (PMC) to identify those that indicate data were submitted to a known repository. After excluding those articles, we analyzed a random sample of the remaining articles to estimate how many and what types of invisible datasets were used in each article.&lt;h4&gt;Results&lt;/h4&gt;About 12% of the articles explicitly mention deposition of datasets in recognized repositories, leaving 88% that are invisible datasets. Among articles with invisible datasets, we found an average of 2.9 to 3.4 datasets, suggesting there were approximately 200,000 to 235,000 invisible datasets generated from NIH-funded research published in 2011. Approximately 87% of the invisible datasets consist of data newly collected for the research reported; 13% reflect reuse of existing data. More than 50% of the datasets were derived from live human or non-human animal subjects.&lt;h4&gt;Conclusion&lt;/h4&gt;In addition to providing a rough estimate of the total number of datasets produced per year by NIH-funded researchers, this study identifies additional issues that must be addressed to improve the discoverability of and access to biomedical research data: the definition of a "dataset," determination of which (if any) data are valuable for archiving and preservation, and better methods for estimating the number of datasets of interest. Lack of consensus amongst annotators about the number of datasets in a given article reinforces the need for a principled way of thinking about how to identify and characterize biomedical datasets.'> Abstract: 293 words, </span></li><li><span class="pmcid"> PMC4574036, </span><span class="author_string"> Pang C, Sollie A, Sijtsma A, Hendriksen D, Charbon B, de Haan M, de Boer T, Kelpin F, Jetten J, van der Velde JK, Smidt N, Sijmons R, Hillege H, Swertz MA., </span><span class="title"> "SORTA: a system for ontology-based re-coding and technical annotation of biomedical phenotype data.", </span><span><b> 2015 </b></span><span> (2015): </span><span><i> Database : the journal of biological databases and curation, </i></span><span class="page_info">PI bav089, </span><a href="https://doi.org/10.1093/database/bav089"> DOI: 10.1093/database/bav089, </a><span class="abstract_text" title="There is an urgent need to standardize the semantics of biomedical data values, such as phenotypes, to enable comparative and integrative analyses. However, it is unlikely that all studies will use the same data collection protocols. As a result, retrospective standardization is often required, which involves matching of original (unstructured or locally coded) data to widely used coding or ontology systems such as SNOMED CT (clinical terms), ICD-10 (International Classification of Disease) and HPO (Human Phenotype Ontology). This data curation process is usually a time-consuming process performed by a human expert. To help mechanize this process, we have developed SORTA, a computer-aided system for rapidly encoding free text or locally coded values to a formal coding system or ontology. SORTA matches original data values (uploaded in semicolon delimited format) to a target coding system (uploaded in Excel spreadsheet, OWL ontology web language or OBO open biomedical ontologies format). It then semi- automatically shortlists candidate codes for each data value using Lucene and n-gram based matching algorithms, and can also learn from matches chosen by human experts. We evaluated SORTA's applicability in two use cases. For the LifeLines biobank, we used SORTA to recode 90&#8201;000 free text values (including 5211 unique values) about physical exercise to MET (Metabolic Equivalent of Task) codes. For the CINEAS clinical symptom coding system, we used SORTA to map to HPO, enriching HPO when necessary (315 terms matched so far). Out of the shortlists at rank 1, we found a precision/recall of 0.97/0.98 in LifeLines and of 0.58/0.45 in CINEAS. More importantly, users found the tool both a major time saver and a quality improvement because SORTA reduced the chances of human mistakes. Thus, SORTA can dramatically ease data (re)coding tasks and we believe it will prove useful for many more projects. Database URL: http://molgenis.org/sorta or as an open source download from http://www.molgenis.org/wiki/SORTA."> Abstract: 308 words, </span></li><li><span class="pmcid"> PMC3884755, </span><span class="author_string"> Frey JG, Bird CL., </span><span class="title"> "Cheminformatics and the Semantic Web: adding value with linked data and enhanced provenance.", </span><span><b> 3(5) </b></span><span> (2013): </span><span><i> Wiley interdisciplinary reviews. Computational molecular science, </i></span><span class="page_info">PI 465-481, </span><a href="https://doi.org/10.1002/wcms.1127"> DOI: 10.1002/wcms.1127, </a><span class="abstract_text" title="Cheminformatics is evolving from being a field of study associated primarily with drug discovery into a discipline that embraces the distribution, management, access, and sharing of chemical data. The relationship with the related subject of bioinformatics is becoming stronger and better defined, owing to the influence of Semantic Web technologies, which enable researchers to integrate heterogeneous sources of chemical, biochemical, biological, and medical information. These developments depend on a range of factors: the principles of chemical identifiers and their role in relationships between chemical and biological entities; the importance of preserving provenance and properly curated metadata; and an understanding of the contribution that the Semantic Web can make at all stages of the research lifecycle. The movements toward open access, open source, and open collaboration all contribute to progress toward the goals of integration."> Abstract: 134 words, </span></li><li><span class="pmcid"> PMC3327820, </span><span class="author_string"> Riggs ER, Jackson L, Miller DT, Van Vooren S., </span><span class="title"> "Phenotypic information in genomic variant databases enhances clinical care and research: the International Standards for Cytogenomic Arrays Consortium experience.", </span><span><b> 33(5) </b></span><span> (2012): </span><span><i> Human mutation, </i></span><span class="page_info">PI 787-796, </span><a href="https://doi.org/10.1002/humu.22052"> DOI: 10.1002/humu.22052, </a><span class="abstract_text" title="Whole-genome analysis, now including whole-genome sequencing, is moving rapidly into the clinical setting, leading to detection of human variation on a broader scale than ever before. Interpreting this information will depend on the availability of thorough and accurate phenotype information, and the ability to curate, store, and access data on genotype-phenotype relationships. This idea has already been demonstrated within the context of chromosomal microarray (CMA) testing. The International Standards for Cytogenomic Arrays (ISCA) Consortium promotes standardization of variant interpretation for this technology through its initiatives, including the formation of a publicly available database housing clinical CMA data. Recognizing that phenotypic data are essential for the interpretation of genomic variants, the ISCA Consortium has developed tools to facilitate the collection of these data and its deposition in a standardized structured format within the ISCA Consortium database. This rich source of phenotypic data can also be used within broader applications such as developing phenotypic profiles of emerging genomic disorders, identification of candidate regions for particular phenotypes, or creation of tools for use in clinical practice. We summarize the ISCA experience as a model for ongoing efforts incorporating phenotype data with genotype data to improve the quality of research and clinical care in human genetics."> Abstract: 202 words, </span></li><li><span class="pmcid"> PMC4110348, </span><span class="author_string"> Cases M, Furlong LI, Albanell J, Altman RB, Bellazzi R, Boyer S, Brand A, Brookes AJ, Brunak S, Clark TW, Gea J, Ghazal P, Graf N, Guig&#243; R, Klein TE, L&#243;pez-Bigas N, Maojo V, Mons B, Musen M, Oliveira JL, Rowe A, Ruch P, Shabo A, Shortliffe EH, Valencia A, van der Lei J, Mayer MA, Sanz F., </span><span class="title"> "Improving data and knowledge management to better integrate health care and research.", </span><span><b> 274(4) </b></span><span> (2013): </span><span><i> Journal of internal medicine, </i></span><span class="page_info">PI 321-328, </span><a href="https://doi.org/10.1111/joim.12105"> DOI: 10.1111/joim.12105, </a></li><li><span class="pmcid"> PMC7098809, </span><span class="author_string"> Harland L, Larminie C, Sansone SA, Popa S, Marshall MS, Braxenthaler M, Cantor M, Filsell W, Forster MJ, Huang E, Matern A, Musen M, Saric J, Slater T, Wilson J, Lynch N, Wise J, Dix I., </span><span class="title"> "Empowering industrial research with shared biomedical vocabularies.", </span><span><b> 16(21-22) </b></span><span> (2011): </span><span><i> Drug discovery today, </i></span><span class="page_info">PI 940-947, </span><a href="https://doi.org/10.1016/j.drudis.2011.09.013"> DOI: 10.1016/j.drudis.2011.09.013, </a><span class="abstract_text" title="The life science industries (including pharmaceuticals, agrochemicals and consumer goods) are exploring new business models for research and development that focus on external partnerships. In parallel, there is a desire to make better use of data obtained from sources such as human clinical samples to inform and support early research programmes. Success in both areas depends upon the successful integration of heterogeneous data from multiple providers and scientific domains, something that is already a major challenge within the industry. This issue is exacerbated by the absence of agreed standards that unambiguously identify the entities, processes and observations within experimental results. In this article we highlight the risks to future productivity that are associated with incomplete biological and chemical vocabularies and suggest a new model to address this long-standing issue."> Abstract: 129 words, </span></li><li><span class="pmcid"> PMC3552778, </span><span class="author_string"> Murray-Rust P, Rzepa HS., </span><span class="title"> "Semantic physical science.", </span><span><b> 4(1) </b></span><span> (2012): </span><span><i> Journal of cheminformatics, </i></span><span class="page_info">PI 14, </span><a href="https://doi.org/10.1186/1758-2946-4-14"> DOI: 10.1186/1758-2946-4-14, </a><span class="abstract_text" title="The articles in this special issue arise from a workshop and symposium held in January 2012 (Semantic Physical Science'). We invited people who shared our vision for the potential of the web to support chemical and related subjects. Other than the initial invitations, we have not exercised any control over the content of the contributed articles."> Abstract: 56 words, </span></li><li><span class="pmcid"> PMC2805925, </span><span class="author_string"> Attwood TK, Kell DB, McDermott P, Marsh J, Pettifer SR, Thorne D., </span><span class="title"> "Calling International Rescue: knowledge lost in literature and data landslide!", </span><span><b> 424(3) </b></span><span> (2009): </span><span><i> The Biochemical journal, </i></span><span class="page_info">PI 317-333, </span><a href="https://doi.org/10.1042/bj20091474"> DOI: 10.1042/bj20091474, </a><span class="abstract_text" title="We live in interesting times. Portents of impending catastrophe pervade the literature, calling us to action in the face of unmanageable volumes of scientific data. But it isn't so much data generation per se, but the systematic burial of the knowledge embodied in those data that poses the problem: there is so much information available that we simply no longer know what we know, and finding what we want is hard - too hard. The knowledge we seek is often fragmentary and disconnected, spread thinly across thousands of databases and millions of articles in thousands of journals. The intellectual energy required to search this array of data-archives, and the time and money this wastes, has led several researchers to challenge the methods by which we traditionally commit newly acquired facts and knowledge to the scientific record. We present some of these initiatives here - a whirlwind tour of recent projects to transform scholarly publishing paradigms, culminating in Utopia and the Semantic Biochemical Journal experiment. With their promises to provide new ways of interacting with the literature, and new and more powerful tools to access and extract the knowledge sequestered within it, we ask what advances they make and what obstacles to progress still exist? We explore these questions, and, as you read on, we invite you to engage in an experiment with us, a real-time test of a new technology to rescue data from the dormant pages of published documents. We ask you, please, to read the instructions carefully. The time has come: you may turn over your papers..."> Abstract: 259 words, </span></li><li><span class="pmcid"> PMC3888109, </span><span class="author_string"> Hoehndorf R, Dumontier M, Gkoutos GV., </span><span class="title"> "Evaluation of research in biomedical ontologies.", </span><span><b> 14(6) </b></span><span> (2013): </span><span><i> Briefings in bioinformatics, </i></span><span class="page_info">PI 696-712, </span><a href="https://doi.org/10.1093/bib/bbs053"> DOI: 10.1093/bib/bbs053, </a><span class="abstract_text" title="Ontologies are now pervasive in biomedicine, where they serve as a means to standardize terminology, to enable access to domain knowledge, to verify data consistency and to facilitate integrative analyses over heterogeneous biomedical data. For this purpose, research on biomedical ontologies applies theories and methods from diverse disciplines such as information management, knowledge representation, cognitive science, linguistics and philosophy. Depending on the desired applications in which ontologies are being applied, the evaluation of research in biomedical ontologies must follow different strategies. Here, we provide a classification of research problems in which ontologies are being applied, focusing on the use of ontologies in basic and translational research, and we demonstrate how research results in biomedical ontologies can be evaluated. The evaluation strategies depend on the desired application and measure the success of using an ontology for a particular biomedical problem. For many applications, the success can be quantified, thereby facilitating the objective evaluation and comparison of research in biomedical ontology. The objective, quantifiable comparison of research results based on scientific applications opens up the possibility for systematically improving the utility of ontologies in biomedical research."> Abstract: 184 words, </span></li><li><span class="pmcid"> PMC3703255, </span><span class="author_string"> Stobbe MD, Swertz MA, Thiele I, Rengaw T, van Kampen AH, Moerland PD., </span><span class="title"> "Consensus and conflict cards for metabolic pathway databases.", </span><span><b> 7 </b></span><span> (2013): </span><span><i> BMC systems biology, </i></span><span class="page_info">PI 50, </span><a href="https://doi.org/10.1186/1752-0509-7-50"> DOI: 10.1186/1752-0509-7-50, </a><span class="abstract_text" title="&lt;h4&gt;Background&lt;/h4&gt;The metabolic network of H. sapiens and many other organisms is described in multiple pathway databases. The level of agreement between these descriptions, however, has proven to be low. We can use these different descriptions to our advantage by identifying conflicting information and combining their knowledge into a single, more accurate, and more complete description. This task is, however, far from trivial.&lt;h4&gt;Results&lt;/h4&gt;We introduce the concept of Consensus and Conflict Cards (C&#8322;Cards) to provide concise overviews of what the databases do or do not agree on. Each card is centered at a single gene, EC number or reaction. These three complementary perspectives make it possible to distinguish disagreements on the underlying biology of a metabolic process from differences that can be explained by different decisions on how and in what detail to represent knowledge. As a proof-of-concept, we implemented C&#8322;Cards(Human), as a web application http://www.molgenis.org/c2cards, covering five human pathway databases.&lt;h4&gt;Conclusions&lt;/h4&gt;C&#8322;Cards can contribute to ongoing reconciliation efforts by simplifying the identification of consensus and conflicts between pathway databases and lowering the threshold for experts to contribute. Several case studies illustrate the potential of the C&#8322;Cards in identifying disagreements on the underlying biology of a metabolic process. The overviews may also point out controversial biological knowledge that should be subject of further research. Finally, the examples provided emphasize the importance of manual curation and the need for a broad community involvement."> Abstract: 228 words, </span></li><li><span class="pmcid"> PMC4028152, </span><span class="author_string"> Keator DB, Helmer K, Steffener J, Turner JA, Van Erp TG, Gadde S, Ashish N, Burns GA, Nichols BN., </span><span class="title"> "Towards structured sharing of raw and derived neuroimaging data across existing resources.", </span><span><b> 82 </b></span><span> (2013): </span><span><i> NeuroImage, </i></span><span class="page_info">PI 647-661, </span><a href="https://doi.org/10.1016/j.neuroimage.2013.05.094"> DOI: 10.1016/j.neuroimage.2013.05.094, </a><span class="abstract_text" title="Data sharing efforts increasingly contribute to the acceleration of scientific discovery. Neuroimaging data is accumulating in distributed domain-specific databases and there is currently no integrated access mechanism nor an accepted format for the critically important meta-data that is necessary for making use of the combined, available neuroimaging data. In this manuscript, we present work from the Derived Data Working Group, an open-access group sponsored by the Biomedical Informatics Research Network (BIRN) and the International Neuroimaging Coordinating Facility (INCF) focused on practical tools for distributed access to neuroimaging data. The working group develops models and tools facilitating the structured interchange of neuroimaging meta-data and is making progress towards a unified set of tools for such data and meta-data exchange. We report on the key components required for integrated access to raw and derived neuroimaging data as well as associated meta-data and provenance across neuroimaging resources. The components include (1) a structured terminology that provides semantic context to data, (2) a formal data model for neuroimaging with robust tracking of data provenance, (3) a web service-based application programming interface (API) that provides a consistent mechanism to access and query the data model, and (4) a provenance library that can be used for the extraction of provenance data by image analysts and imaging software developers. We believe that the framework and set of tools outlined in this manuscript have great potential for solving many of the issues the neuroimaging community faces when sharing raw and derived neuroimaging data across the various existing database systems for the purpose of accelerating scientific discovery."> Abstract: 258 words, </span></li></ul></body></html>